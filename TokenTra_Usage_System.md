# TokenTra Usage System

## Complete Enterprise Specification for AI Consumption Analytics & Tracking

**Version:** 1.0  
**Last Updated:** December 2025  
**Status:** Production Ready

---

## Table of Contents

1. [Overview](#1-overview)
2. [Database Schema](#2-database-schema)
3. [TypeScript Types](#3-typescript-types)
4. [Usage Dashboard Service](#4-usage-dashboard-service)
5. [Usage Analytics Engine](#5-usage-analytics-engine)
6. [Usage Breakdown Service](#6-usage-breakdown-service)
7. [Usage Trends Service](#7-usage-trends-service)
8. [Usage Quotas & Limits](#8-usage-quotas--limits)
9. [Usage Attribution Service](#9-usage-attribution-service)
10. [Real-Time Usage Monitoring](#10-real-time-usage-monitoring)
11. [Usage Comparison Service](#11-usage-comparison-service)
12. [Usage Export Service](#12-usage-export-service)
13. [API Routes](#13-api-routes)
14. [React Hooks](#14-react-hooks)
15. [Dashboard Components](#15-dashboard-components)
16. [Background Jobs](#16-background-jobs)

---

## 1. Overview

### 1.1 Purpose

The Usage System is TokenTra's consumption intelligence layer that provides granular visibility into AI API usage patterns. While the Cost Analysis System focuses on financial metrics (dollars, budgets, chargebacks), the Usage System focuses on **consumption metrics**: tokens, requests, API calls, and operational patterns.

### 1.2 Usage vs Cost Analysis

| Aspect | Usage System | Cost Analysis |
|--------|--------------|---------------|
| **Primary Focus** | Consumption (tokens, requests) | Financial (dollars, budgets) |
| **Key Metrics** | Token counts, request counts, latency | Total cost, cost per unit, savings |
| **Questions Answered** | "How much are we using?" | "How much are we spending?" |
| **User Goals** | Optimize consumption, set quotas | Control budget, reduce spend |
| **Time Granularity** | Minute-level, request-level | Hourly, daily aggregations |
| **Attribution** | Per-request, per-user | Per-team, per-cost-center |

### 1.3 Key Capabilities

| Capability | Description | Business Value |
|------------|-------------|----------------|
| **Real-time Tracking** | Usage updated every minute | Immediate visibility |
| **Request-level Detail** | Individual API call tracking | Debugging & optimization |
| **Token Analytics** | Input/output/cached token breakdown | Prompt optimization |
| **User Activity** | Per-user consumption patterns | Usage accountability |
| **Model Distribution** | Usage by model breakdown | Model selection insights |
| **Quota Management** | Usage limits per dimension | Cost control |
| **Trend Analysis** | Historical usage patterns | Capacity planning |
| **Performance Metrics** | Latency, error rates | Quality monitoring |

### 1.4 Core Usage Metrics

| Metric | Type | Description |
|--------|------|-------------|
| `totalRequests` | Integer | Total API calls made |
| `inputTokens` | Integer | Tokens sent to models |
| `outputTokens` | Integer | Tokens generated by models |
| `totalTokens` | Integer | inputTokens + outputTokens |
| `cachedTokens` | Integer | Tokens served from cache |
| `cacheCreationTokens` | Integer | Tokens used to create cache |
| `averageLatency` | Milliseconds | Mean response time |
| `p50Latency` | Milliseconds | Median response time |
| `p95Latency` | Milliseconds | 95th percentile response time |
| `p99Latency` | Milliseconds | 99th percentile response time |
| `errorCount` | Integer | Failed requests |
| `errorRate` | Percentage | errorCount / totalRequests |
| `successRate` | Percentage | 100 - errorRate |
| `tokensPerRequest` | Float | Average tokens per call |
| `requestsPerMinute` | Float | Request throughput |
| `activeUsers` | Integer | Unique users in period |
| `activeFeatures` | Integer | Features with activity |

### 1.5 Dimensions

| Dimension | Description | Use Case |
|-----------|-------------|----------|
| `provider` | AI provider (OpenAI, Anthropic, etc.) | Provider comparison |
| `model` | Specific model version | Model optimization |
| `modelFamily` | Model family (GPT-4, Claude, etc.) | Family trends |
| `team` | Team identifier | Team accountability |
| `project` | Project identifier | Project tracking |
| `feature` | Feature/product area | Feature usage |
| `environment` | prod, staging, dev, test | Environment monitoring |
| `user` | End user identifier | User behavior |
| `apiKey` | API key used | Key management |
| `requestType` | chat, completion, embedding, etc. | Type analysis |
| `serviceType` | sync, batch, real-time | Service patterns |

---

## 2. Database Schema

### 2.1 Core Usage Records Table

```sql
-- ============================================
-- USAGE RECORDS TABLE (Primary Storage)
-- ============================================
-- Stores individual usage records from provider syncs
-- Time-series optimized with partitioning

CREATE TABLE usage_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  org_id UUID NOT NULL REFERENCES organizations(id),
  
  -- Provider identification
  provider_connection_id UUID NOT NULL REFERENCES provider_connections(id),
  provider TEXT NOT NULL, -- 'openai', 'anthropic', 'google', 'azure', 'aws'
  
  -- Timing
  timestamp TIMESTAMPTZ NOT NULL,
  bucket_start TIMESTAMPTZ NOT NULL, -- Start of aggregation bucket
  bucket_end TIMESTAMPTZ NOT NULL,   -- End of aggregation bucket
  granularity TEXT NOT NULL DEFAULT '1m', -- '1m', '1h', '1d'
  
  -- Model information
  model TEXT NOT NULL,
  model_family TEXT,
  request_type TEXT, -- 'chat', 'completion', 'embedding', 'image', 'audio'
  service_tier TEXT, -- 'default', 'scale', 'batch'
  
  -- Token metrics
  input_tokens BIGINT NOT NULL DEFAULT 0,
  output_tokens BIGINT NOT NULL DEFAULT 0,
  total_tokens BIGINT GENERATED ALWAYS AS (input_tokens + output_tokens) STORED,
  cached_input_tokens BIGINT DEFAULT 0,
  cache_creation_tokens BIGINT DEFAULT 0,
  
  -- Request metrics
  request_count INTEGER NOT NULL DEFAULT 0,
  success_count INTEGER NOT NULL DEFAULT 0,
  error_count INTEGER NOT NULL DEFAULT 0,
  
  -- Performance metrics (in milliseconds)
  total_latency_ms BIGINT DEFAULT 0,
  min_latency_ms INTEGER,
  max_latency_ms INTEGER,
  p50_latency_ms INTEGER,
  p95_latency_ms INTEGER,
  p99_latency_ms INTEGER,
  
  -- Cost (for reference, primary cost tracking in cost_analysis)
  estimated_cost DECIMAL(12, 6) DEFAULT 0,
  
  -- Attribution (from SDK tags or auto-detection)
  team_id UUID REFERENCES teams(id),
  project_id UUID REFERENCES projects(id),
  cost_center_id UUID REFERENCES cost_centers(id),
  feature TEXT,
  environment TEXT DEFAULT 'production',
  api_key_id TEXT,
  
  -- User tracking
  user_ids TEXT[], -- Array of user IDs in this bucket
  unique_user_count INTEGER DEFAULT 0,
  
  -- Provider-specific metadata
  provider_metadata JSONB DEFAULT '{}',
  
  -- Sync tracking
  sync_id UUID REFERENCES sync_history(id),
  synced_at TIMESTAMPTZ DEFAULT NOW(),
  
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- Constraints
  CONSTRAINT valid_provider CHECK (provider IN ('openai', 'anthropic', 'google', 'azure', 'aws')),
  CONSTRAINT valid_granularity CHECK (granularity IN ('1m', '1h', '1d')),
  CONSTRAINT non_negative_tokens CHECK (input_tokens >= 0 AND output_tokens >= 0),
  CONSTRAINT non_negative_requests CHECK (request_count >= 0)
);

-- ============================================
-- PARTITIONING BY TIME (Monthly)
-- ============================================
-- Partition by month for efficient querying and retention

CREATE TABLE usage_records_y2025m01 PARTITION OF usage_records
  FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
CREATE TABLE usage_records_y2025m02 PARTITION OF usage_records
  FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
-- ... continue for each month

-- ============================================
-- INDEXES
-- ============================================

-- Primary query pattern: org + time range
CREATE INDEX idx_usage_org_time ON usage_records (org_id, timestamp DESC);

-- Provider breakdown
CREATE INDEX idx_usage_provider ON usage_records (org_id, provider, timestamp DESC);

-- Model analysis
CREATE INDEX idx_usage_model ON usage_records (org_id, model, timestamp DESC);
CREATE INDEX idx_usage_model_family ON usage_records (org_id, model_family, timestamp DESC);

-- Attribution dimensions
CREATE INDEX idx_usage_team ON usage_records (team_id, timestamp DESC) WHERE team_id IS NOT NULL;
CREATE INDEX idx_usage_project ON usage_records (project_id, timestamp DESC) WHERE project_id IS NOT NULL;
CREATE INDEX idx_usage_cost_center ON usage_records (cost_center_id, timestamp DESC) WHERE cost_center_id IS NOT NULL;
CREATE INDEX idx_usage_feature ON usage_records (org_id, feature, timestamp DESC) WHERE feature IS NOT NULL;

-- Environment filtering
CREATE INDEX idx_usage_environment ON usage_records (org_id, environment, timestamp DESC);

-- Request type analysis
CREATE INDEX idx_usage_request_type ON usage_records (org_id, request_type, timestamp DESC);

-- Hot data partial index (last 7 days)
CREATE INDEX idx_usage_recent ON usage_records (org_id, timestamp DESC)
  WHERE timestamp > NOW() - INTERVAL '7 days';

-- Composite for dashboard queries
CREATE INDEX idx_usage_dashboard ON usage_records (
  org_id, timestamp DESC, provider, model
) INCLUDE (input_tokens, output_tokens, request_count);
```

### 2.2 Hourly Usage Rollup

```sql
-- ============================================
-- HOURLY USAGE ROLLUP (30-day retention)
-- ============================================
-- Pre-aggregated hourly usage for fast dashboard queries

CREATE MATERIALIZED VIEW hourly_usage_rollup AS
SELECT 
  org_id,
  date_trunc('hour', timestamp) AS hour,
  provider,
  model,
  model_family,
  request_type,
  team_id,
  project_id,
  cost_center_id,
  feature,
  environment,
  
  -- Token aggregations
  SUM(input_tokens)::BIGINT AS input_tokens,
  SUM(output_tokens)::BIGINT AS output_tokens,
  SUM(total_tokens)::BIGINT AS total_tokens,
  SUM(cached_input_tokens)::BIGINT AS cached_input_tokens,
  SUM(cache_creation_tokens)::BIGINT AS cache_creation_tokens,
  
  -- Request aggregations
  SUM(request_count)::INTEGER AS request_count,
  SUM(success_count)::INTEGER AS success_count,
  SUM(error_count)::INTEGER AS error_count,
  
  -- Latency aggregations (weighted average)
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(total_latency_ms)::FLOAT / SUM(request_count))::INTEGER 
    ELSE NULL 
  END AS avg_latency_ms,
  MIN(min_latency_ms) AS min_latency_ms,
  MAX(max_latency_ms) AS max_latency_ms,
  
  -- Percentile approximations (weighted)
  PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY p50_latency_ms) AS p50_latency_ms,
  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY p95_latency_ms) AS p95_latency_ms,
  PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY p99_latency_ms) AS p99_latency_ms,
  
  -- Cost aggregation
  SUM(estimated_cost)::DECIMAL(12, 6) AS estimated_cost,
  
  -- User aggregations
  SUM(unique_user_count)::INTEGER AS total_user_count,
  
  -- Computed metrics
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(total_tokens)::FLOAT / SUM(request_count))::FLOAT 
    ELSE 0 
  END AS avg_tokens_per_request,
  
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(error_count)::FLOAT / SUM(request_count) * 100)::FLOAT 
    ELSE 0 
  END AS error_rate_pct,
  
  CASE 
    WHEN SUM(input_tokens) > 0 
    THEN (SUM(cached_input_tokens)::FLOAT / SUM(input_tokens) * 100)::FLOAT 
    ELSE 0 
  END AS cache_hit_rate_pct,
  
  -- Record count for reference
  COUNT(*)::INTEGER AS record_count
  
FROM usage_records
WHERE timestamp > NOW() - INTERVAL '30 days'
GROUP BY 
  org_id,
  date_trunc('hour', timestamp),
  provider,
  model,
  model_family,
  request_type,
  team_id,
  project_id,
  cost_center_id,
  feature,
  environment;

-- Unique index for concurrent refresh
CREATE UNIQUE INDEX idx_hourly_usage_unique ON hourly_usage_rollup (
  org_id, hour, provider, model, COALESCE(model_family, ''), 
  COALESCE(request_type, ''), COALESCE(team_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(project_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(cost_center_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(feature, ''), COALESCE(environment, '')
);

-- Query indexes
CREATE INDEX idx_hourly_usage_org ON hourly_usage_rollup (org_id, hour DESC);
CREATE INDEX idx_hourly_usage_provider ON hourly_usage_rollup (org_id, provider, hour DESC);
CREATE INDEX idx_hourly_usage_model ON hourly_usage_rollup (org_id, model, hour DESC);
```

### 2.3 Daily Usage Rollup

```sql
-- ============================================
-- DAILY USAGE ROLLUP (2-year retention)
-- ============================================

CREATE MATERIALIZED VIEW daily_usage_rollup AS
SELECT 
  org_id,
  date_trunc('day', timestamp)::DATE AS date,
  provider,
  model,
  model_family,
  request_type,
  team_id,
  project_id,
  cost_center_id,
  feature,
  environment,
  
  -- Token aggregations
  SUM(input_tokens)::BIGINT AS input_tokens,
  SUM(output_tokens)::BIGINT AS output_tokens,
  SUM(total_tokens)::BIGINT AS total_tokens,
  SUM(cached_input_tokens)::BIGINT AS cached_input_tokens,
  SUM(cache_creation_tokens)::BIGINT AS cache_creation_tokens,
  
  -- Request aggregations
  SUM(request_count)::INTEGER AS request_count,
  SUM(success_count)::INTEGER AS success_count,
  SUM(error_count)::INTEGER AS error_count,
  
  -- Latency aggregations
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(total_latency_ms)::FLOAT / SUM(request_count))::INTEGER 
    ELSE NULL 
  END AS avg_latency_ms,
  MIN(min_latency_ms) AS min_latency_ms,
  MAX(max_latency_ms) AS max_latency_ms,
  
  -- Cost aggregation
  SUM(estimated_cost)::DECIMAL(12, 6) AS estimated_cost,
  
  -- User aggregations (approximate unique across day)
  SUM(unique_user_count)::INTEGER AS total_user_count,
  
  -- Computed metrics
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(total_tokens)::FLOAT / SUM(request_count))::FLOAT 
    ELSE 0 
  END AS avg_tokens_per_request,
  
  CASE 
    WHEN SUM(request_count) > 0 
    THEN (SUM(error_count)::FLOAT / SUM(request_count) * 100)::FLOAT 
    ELSE 0 
  END AS error_rate_pct,
  
  CASE 
    WHEN SUM(input_tokens) > 0 
    THEN (SUM(cached_input_tokens)::FLOAT / SUM(input_tokens) * 100)::FLOAT 
    ELSE 0 
  END AS cache_hit_rate_pct,
  
  -- Peak metrics
  MAX(request_count) AS peak_hourly_requests,
  MAX(total_tokens) AS peak_hourly_tokens,
  
  COUNT(*)::INTEGER AS record_count
  
FROM usage_records
WHERE timestamp > NOW() - INTERVAL '2 years'
GROUP BY 
  org_id,
  date_trunc('day', timestamp)::DATE,
  provider,
  model,
  model_family,
  request_type,
  team_id,
  project_id,
  cost_center_id,
  feature,
  environment;

-- Unique index for concurrent refresh
CREATE UNIQUE INDEX idx_daily_usage_unique ON daily_usage_rollup (
  org_id, date, provider, model, COALESCE(model_family, ''), 
  COALESCE(request_type, ''), COALESCE(team_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(project_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(cost_center_id, '00000000-0000-0000-0000-000000000000'::UUID),
  COALESCE(feature, ''), COALESCE(environment, '')
);

-- Query indexes
CREATE INDEX idx_daily_usage_org ON daily_usage_rollup (org_id, date DESC);
CREATE INDEX idx_daily_usage_provider ON daily_usage_rollup (org_id, provider, date DESC);
CREATE INDEX idx_daily_usage_model ON daily_usage_rollup (org_id, model, date DESC);
CREATE INDEX idx_daily_usage_team ON daily_usage_rollup (team_id, date DESC) WHERE team_id IS NOT NULL;
```

### 2.4 Usage Quotas Table

```sql
-- ============================================
-- USAGE QUOTAS TABLE
-- ============================================
-- Define usage limits per dimension

CREATE TABLE usage_quotas (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  org_id UUID NOT NULL REFERENCES organizations(id),
  
  -- Quota identification
  name TEXT NOT NULL,
  description TEXT,
  
  -- Target dimension (only one should be set)
  scope_type TEXT NOT NULL, -- 'organization', 'team', 'project', 'user', 'api_key', 'feature'
  team_id UUID REFERENCES teams(id),
  project_id UUID REFERENCES projects(id),
  user_identifier TEXT,
  api_key_id TEXT,
  feature TEXT,
  
  -- Quota limits (NULL = unlimited)
  token_limit BIGINT, -- Max tokens per period
  request_limit INTEGER, -- Max requests per period
  input_token_limit BIGINT, -- Max input tokens
  output_token_limit BIGINT, -- Max output tokens
  
  -- Period configuration
  period_type TEXT NOT NULL DEFAULT 'monthly', -- 'hourly', 'daily', 'weekly', 'monthly'
  period_reset_day INTEGER DEFAULT 1, -- Day of month/week to reset
  period_reset_hour INTEGER DEFAULT 0, -- Hour of day to reset (UTC)
  
  -- Current usage tracking (updated periodically)
  current_tokens BIGINT DEFAULT 0,
  current_requests INTEGER DEFAULT 0,
  current_input_tokens BIGINT DEFAULT 0,
  current_output_tokens BIGINT DEFAULT 0,
  current_period_start TIMESTAMPTZ,
  current_period_end TIMESTAMPTZ,
  
  -- Enforcement settings
  enforcement_mode TEXT NOT NULL DEFAULT 'alert', -- 'alert', 'throttle', 'block'
  warning_threshold_pct INTEGER DEFAULT 80, -- Alert at this % of limit
  critical_threshold_pct INTEGER DEFAULT 95, -- Critical alert at this %
  
  -- Status
  is_active BOOLEAN DEFAULT true,
  
  -- Metadata
  created_by UUID REFERENCES users(id),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- Constraints
  CONSTRAINT valid_scope_type CHECK (scope_type IN ('organization', 'team', 'project', 'user', 'api_key', 'feature')),
  CONSTRAINT valid_period_type CHECK (period_type IN ('hourly', 'daily', 'weekly', 'monthly')),
  CONSTRAINT valid_enforcement CHECK (enforcement_mode IN ('alert', 'throttle', 'block')),
  CONSTRAINT has_at_least_one_limit CHECK (
    token_limit IS NOT NULL OR 
    request_limit IS NOT NULL OR 
    input_token_limit IS NOT NULL OR 
    output_token_limit IS NOT NULL
  )
);

-- Indexes
CREATE INDEX idx_quotas_org ON usage_quotas (org_id) WHERE is_active = true;
CREATE INDEX idx_quotas_team ON usage_quotas (team_id) WHERE team_id IS NOT NULL AND is_active = true;
CREATE INDEX idx_quotas_project ON usage_quotas (project_id) WHERE project_id IS NOT NULL AND is_active = true;
CREATE INDEX idx_quotas_user ON usage_quotas (org_id, user_identifier) WHERE user_identifier IS NOT NULL AND is_active = true;
CREATE INDEX idx_quotas_api_key ON usage_quotas (org_id, api_key_id) WHERE api_key_id IS NOT NULL AND is_active = true;
```

### 2.5 Quota Usage History Table

```sql
-- ============================================
-- QUOTA USAGE HISTORY
-- ============================================
-- Historical tracking of quota utilization per period

CREATE TABLE quota_usage_history (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  quota_id UUID NOT NULL REFERENCES usage_quotas(id) ON DELETE CASCADE,
  org_id UUID NOT NULL REFERENCES organizations(id),
  
  -- Period
  period_start TIMESTAMPTZ NOT NULL,
  period_end TIMESTAMPTZ NOT NULL,
  
  -- Usage for this period
  total_tokens BIGINT NOT NULL DEFAULT 0,
  total_requests INTEGER NOT NULL DEFAULT 0,
  input_tokens BIGINT NOT NULL DEFAULT 0,
  output_tokens BIGINT NOT NULL DEFAULT 0,
  
  -- Limit values at time of period (for historical reference)
  token_limit BIGINT,
  request_limit INTEGER,
  
  -- Utilization percentages
  token_utilization_pct FLOAT,
  request_utilization_pct FLOAT,
  
  -- Status at period end
  exceeded_limit BOOLEAN DEFAULT false,
  max_utilization_pct FLOAT,
  
  -- Actions taken
  alerts_sent INTEGER DEFAULT 0,
  throttle_events INTEGER DEFAULT 0,
  block_events INTEGER DEFAULT 0,
  
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT unique_quota_period UNIQUE (quota_id, period_start)
);

-- Indexes
CREATE INDEX idx_quota_history_quota ON quota_usage_history (quota_id, period_start DESC);
CREATE INDEX idx_quota_history_org ON quota_usage_history (org_id, period_start DESC);
CREATE INDEX idx_quota_history_exceeded ON quota_usage_history (org_id, exceeded_limit, period_start DESC) 
  WHERE exceeded_limit = true;
```

### 2.6 User Activity Table

```sql
-- ============================================
-- USER ACTIVITY TABLE
-- ============================================
-- Per-user usage tracking for detailed attribution

CREATE TABLE user_activity (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  org_id UUID NOT NULL REFERENCES organizations(id),
  
  -- User identification
  user_identifier TEXT NOT NULL, -- SDK user ID or email
  user_display_name TEXT,
  user_metadata JSONB DEFAULT '{}',
  
  -- Activity date
  activity_date DATE NOT NULL,
  
  -- Usage metrics for the day
  total_tokens BIGINT NOT NULL DEFAULT 0,
  input_tokens BIGINT NOT NULL DEFAULT 0,
  output_tokens BIGINT NOT NULL DEFAULT 0,
  cached_tokens BIGINT DEFAULT 0,
  request_count INTEGER NOT NULL DEFAULT 0,
  error_count INTEGER DEFAULT 0,
  
  -- Cost for the day
  estimated_cost DECIMAL(12, 6) DEFAULT 0,
  
  -- Model distribution (JSONB for flexibility)
  model_usage JSONB DEFAULT '{}', -- {"gpt-4o": {tokens: 1000, requests: 5}, ...}
  
  -- Feature distribution
  feature_usage JSONB DEFAULT '{}', -- {"chat": {tokens: 500}, "search": {tokens: 500}}
  
  -- Session metrics
  session_count INTEGER DEFAULT 1,
  first_activity_at TIMESTAMPTZ,
  last_activity_at TIMESTAMPTZ,
  
  -- Performance
  avg_latency_ms INTEGER,
  
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT unique_user_day UNIQUE (org_id, user_identifier, activity_date)
);

-- Indexes
CREATE INDEX idx_user_activity_org ON user_activity (org_id, activity_date DESC);
CREATE INDEX idx_user_activity_user ON user_activity (org_id, user_identifier, activity_date DESC);
CREATE INDEX idx_user_activity_usage ON user_activity (org_id, total_tokens DESC, activity_date DESC);
```

### 2.7 Stored Functions

```sql
-- ============================================
-- GET USAGE SUMMARY
-- ============================================
CREATE OR REPLACE FUNCTION get_usage_summary(
  p_org_id UUID,
  p_start_date TIMESTAMPTZ,
  p_end_date TIMESTAMPTZ,
  p_provider TEXT DEFAULT NULL,
  p_model TEXT DEFAULT NULL,
  p_team_id UUID DEFAULT NULL,
  p_project_id UUID DEFAULT NULL,
  p_feature TEXT DEFAULT NULL,
  p_environment TEXT DEFAULT NULL
)
RETURNS TABLE (
  total_tokens BIGINT,
  input_tokens BIGINT,
  output_tokens BIGINT,
  cached_tokens BIGINT,
  total_requests INTEGER,
  success_requests INTEGER,
  error_requests INTEGER,
  avg_latency_ms INTEGER,
  p50_latency_ms INTEGER,
  p95_latency_ms INTEGER,
  p99_latency_ms INTEGER,
  error_rate FLOAT,
  cache_hit_rate FLOAT,
  tokens_per_request FLOAT,
  unique_models INTEGER,
  unique_users INTEGER,
  estimated_cost DECIMAL
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    COALESCE(SUM(ur.total_tokens), 0)::BIGINT,
    COALESCE(SUM(ur.input_tokens), 0)::BIGINT,
    COALESCE(SUM(ur.output_tokens), 0)::BIGINT,
    COALESCE(SUM(ur.cached_input_tokens), 0)::BIGINT,
    COALESCE(SUM(ur.request_count), 0)::INTEGER,
    COALESCE(SUM(ur.success_count), 0)::INTEGER,
    COALESCE(SUM(ur.error_count), 0)::INTEGER,
    CASE WHEN SUM(ur.request_count) > 0 
      THEN (SUM(ur.total_latency_ms) / SUM(ur.request_count))::INTEGER 
      ELSE NULL 
    END,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ur.p50_latency_ms)::INTEGER,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY ur.p95_latency_ms)::INTEGER,
    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY ur.p99_latency_ms)::INTEGER,
    CASE WHEN SUM(ur.request_count) > 0 
      THEN (SUM(ur.error_count)::FLOAT / SUM(ur.request_count) * 100)
      ELSE 0 
    END,
    CASE WHEN SUM(ur.input_tokens) > 0 
      THEN (SUM(ur.cached_input_tokens)::FLOAT / SUM(ur.input_tokens) * 100)
      ELSE 0 
    END,
    CASE WHEN SUM(ur.request_count) > 0 
      THEN (SUM(ur.total_tokens)::FLOAT / SUM(ur.request_count))
      ELSE 0 
    END,
    COUNT(DISTINCT ur.model)::INTEGER,
    COALESCE(SUM(ur.unique_user_count), 0)::INTEGER,
    COALESCE(SUM(ur.estimated_cost), 0)::DECIMAL
  FROM usage_records ur
  WHERE ur.org_id = p_org_id
    AND ur.timestamp >= p_start_date
    AND ur.timestamp < p_end_date
    AND (p_provider IS NULL OR ur.provider = p_provider)
    AND (p_model IS NULL OR ur.model = p_model)
    AND (p_team_id IS NULL OR ur.team_id = p_team_id)
    AND (p_project_id IS NULL OR ur.project_id = p_project_id)
    AND (p_feature IS NULL OR ur.feature = p_feature)
    AND (p_environment IS NULL OR ur.environment = p_environment);
END;
$$;

-- ============================================
-- GET USAGE TREND
-- ============================================
CREATE OR REPLACE FUNCTION get_usage_trend(
  p_org_id UUID,
  p_start_date TIMESTAMPTZ,
  p_end_date TIMESTAMPTZ,
  p_granularity TEXT DEFAULT 'day', -- 'hour', 'day', 'week', 'month'
  p_metric TEXT DEFAULT 'tokens' -- 'tokens', 'requests', 'cost'
)
RETURNS TABLE (
  time_bucket TIMESTAMPTZ,
  value BIGINT,
  input_tokens BIGINT,
  output_tokens BIGINT,
  requests INTEGER,
  error_rate FLOAT
)
LANGUAGE plpgsql
AS $$
DECLARE
  v_interval TEXT;
BEGIN
  -- Determine the interval based on granularity
  v_interval := CASE p_granularity
    WHEN 'hour' THEN '1 hour'
    WHEN 'day' THEN '1 day'
    WHEN 'week' THEN '1 week'
    WHEN 'month' THEN '1 month'
    ELSE '1 day'
  END;
  
  RETURN QUERY EXECUTE format('
    SELECT
      date_trunc(%L, timestamp) AS time_bucket,
      CASE %L
        WHEN ''tokens'' THEN SUM(total_tokens)
        WHEN ''requests'' THEN SUM(request_count)::BIGINT
        WHEN ''cost'' THEN (SUM(estimated_cost) * 1000000)::BIGINT -- Microdollars for integer
        ELSE SUM(total_tokens)
      END AS value,
      SUM(input_tokens)::BIGINT,
      SUM(output_tokens)::BIGINT,
      SUM(request_count)::INTEGER,
      CASE WHEN SUM(request_count) > 0 
        THEN (SUM(error_count)::FLOAT / SUM(request_count) * 100)
        ELSE 0 
      END AS error_rate
    FROM usage_records
    WHERE org_id = %L
      AND timestamp >= %L
      AND timestamp < %L
    GROUP BY date_trunc(%L, timestamp)
    ORDER BY time_bucket ASC',
    p_granularity, p_metric, p_org_id, p_start_date, p_end_date, p_granularity
  );
END;
$$;

-- ============================================
-- GET USAGE BREAKDOWN
-- ============================================
CREATE OR REPLACE FUNCTION get_usage_breakdown(
  p_org_id UUID,
  p_start_date TIMESTAMPTZ,
  p_end_date TIMESTAMPTZ,
  p_dimension TEXT, -- 'provider', 'model', 'team', 'project', 'feature', 'user', 'request_type'
  p_limit INTEGER DEFAULT 10
)
RETURNS TABLE (
  dimension_value TEXT,
  total_tokens BIGINT,
  input_tokens BIGINT,
  output_tokens BIGINT,
  request_count INTEGER,
  error_count INTEGER,
  avg_latency_ms INTEGER,
  estimated_cost DECIMAL,
  percentage FLOAT
)
LANGUAGE plpgsql
AS $$
DECLARE
  v_total_tokens BIGINT;
BEGIN
  -- Get total for percentage calculation
  SELECT COALESCE(SUM(ur.total_tokens), 1)
  INTO v_total_tokens
  FROM usage_records ur
  WHERE ur.org_id = p_org_id
    AND ur.timestamp >= p_start_date
    AND ur.timestamp < p_end_date;
  
  RETURN QUERY EXECUTE format('
    SELECT
      COALESCE(%I::TEXT, ''Unknown'') AS dimension_value,
      SUM(total_tokens)::BIGINT,
      SUM(input_tokens)::BIGINT,
      SUM(output_tokens)::BIGINT,
      SUM(request_count)::INTEGER,
      SUM(error_count)::INTEGER,
      CASE WHEN SUM(request_count) > 0 
        THEN (SUM(total_latency_ms) / SUM(request_count))::INTEGER 
        ELSE NULL 
      END,
      SUM(estimated_cost)::DECIMAL,
      (SUM(total_tokens)::FLOAT / %L * 100)::FLOAT AS percentage
    FROM usage_records
    WHERE org_id = %L
      AND timestamp >= %L
      AND timestamp < %L
    GROUP BY %I
    ORDER BY SUM(total_tokens) DESC
    LIMIT %L',
    p_dimension, v_total_tokens, p_org_id, p_start_date, p_end_date, p_dimension, p_limit
  );
END;
$$;

-- ============================================
-- GET TOP USERS
-- ============================================
CREATE OR REPLACE FUNCTION get_top_users(
  p_org_id UUID,
  p_start_date DATE,
  p_end_date DATE,
  p_limit INTEGER DEFAULT 10,
  p_order_by TEXT DEFAULT 'tokens' -- 'tokens', 'requests', 'cost'
)
RETURNS TABLE (
  user_identifier TEXT,
  user_display_name TEXT,
  total_tokens BIGINT,
  request_count INTEGER,
  estimated_cost DECIMAL,
  model_usage JSONB,
  last_activity_at TIMESTAMPTZ
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY EXECUTE format('
    SELECT
      ua.user_identifier,
      ua.user_display_name,
      SUM(ua.total_tokens)::BIGINT,
      SUM(ua.request_count)::INTEGER,
      SUM(ua.estimated_cost)::DECIMAL,
      jsonb_object_agg(
        COALESCE(key, ''unknown''),
        value
      ) FILTER (WHERE key IS NOT NULL) AS model_usage,
      MAX(ua.last_activity_at) AS last_activity_at
    FROM user_activity ua,
         LATERAL jsonb_each(ua.model_usage) AS mu(key, value)
    WHERE ua.org_id = %L
      AND ua.activity_date >= %L
      AND ua.activity_date < %L
    GROUP BY ua.user_identifier, ua.user_display_name
    ORDER BY %s DESC
    LIMIT %L',
    p_org_id, p_start_date, p_end_date,
    CASE p_order_by
      WHEN 'tokens' THEN 'SUM(ua.total_tokens)'
      WHEN 'requests' THEN 'SUM(ua.request_count)'
      WHEN 'cost' THEN 'SUM(ua.estimated_cost)'
      ELSE 'SUM(ua.total_tokens)'
    END,
    p_limit
  );
END;
$$;

-- ============================================
-- CHECK QUOTA STATUS
-- ============================================
CREATE OR REPLACE FUNCTION check_quota_status(
  p_quota_id UUID
)
RETURNS TABLE (
  quota_id UUID,
  quota_name TEXT,
  scope_type TEXT,
  current_tokens BIGINT,
  token_limit BIGINT,
  token_utilization_pct FLOAT,
  current_requests INTEGER,
  request_limit INTEGER,
  request_utilization_pct FLOAT,
  status TEXT, -- 'ok', 'warning', 'critical', 'exceeded'
  period_remaining_hours INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
  v_quota RECORD;
  v_status TEXT;
  v_token_util FLOAT;
  v_request_util FLOAT;
  v_max_util FLOAT;
BEGIN
  SELECT * INTO v_quota FROM usage_quotas WHERE id = p_quota_id;
  
  -- Calculate utilizations
  v_token_util := CASE 
    WHEN v_quota.token_limit IS NOT NULL AND v_quota.token_limit > 0 
    THEN (v_quota.current_tokens::FLOAT / v_quota.token_limit * 100)
    ELSE 0 
  END;
  
  v_request_util := CASE 
    WHEN v_quota.request_limit IS NOT NULL AND v_quota.request_limit > 0 
    THEN (v_quota.current_requests::FLOAT / v_quota.request_limit * 100)
    ELSE 0 
  END;
  
  v_max_util := GREATEST(v_token_util, v_request_util);
  
  -- Determine status
  v_status := CASE
    WHEN v_max_util >= 100 THEN 'exceeded'
    WHEN v_max_util >= v_quota.critical_threshold_pct THEN 'critical'
    WHEN v_max_util >= v_quota.warning_threshold_pct THEN 'warning'
    ELSE 'ok'
  END;
  
  RETURN QUERY SELECT
    v_quota.id,
    v_quota.name,
    v_quota.scope_type,
    v_quota.current_tokens,
    v_quota.token_limit,
    v_token_util,
    v_quota.current_requests,
    v_quota.request_limit,
    v_request_util,
    v_status,
    EXTRACT(EPOCH FROM (v_quota.current_period_end - NOW()))::INTEGER / 3600;
END;
$$;
```

---

## 3. TypeScript Types

```typescript
// ============================================
// CORE USAGE TYPES
// ============================================

export interface UsageMetrics {
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  cachedTokens: number;
  cacheCreationTokens: number;
  totalRequests: number;
  successRequests: number;
  errorRequests: number;
  errorRate: number; // Percentage 0-100
  cacheHitRate: number; // Percentage 0-100
  tokensPerRequest: number;
  requestsPerMinute: number;
}

export interface LatencyMetrics {
  avgLatencyMs: number | null;
  minLatencyMs: number | null;
  maxLatencyMs: number | null;
  p50LatencyMs: number | null;
  p95LatencyMs: number | null;
  p99LatencyMs: number | null;
}

export interface UsageSummary extends UsageMetrics, LatencyMetrics {
  startDate: string;
  endDate: string;
  uniqueModels: number;
  uniqueUsers: number;
  estimatedCost: number;
}

export interface UsageSummaryWithComparison extends UsageSummary {
  comparison?: {
    period: 'previous' | 'mom' | 'qoq' | 'yoy';
    previousValue: UsageSummary;
    changes: {
      tokens: PercentageChange;
      requests: PercentageChange;
      errorRate: PercentageChange;
      latency: PercentageChange;
      cacheHitRate: PercentageChange;
    };
  };
}

export interface PercentageChange {
  absolute: number;
  percentage: number;
  direction: 'up' | 'down' | 'flat';
  isImprovement: boolean; // Context-aware (e.g., lower error rate is improvement)
}

// ============================================
// TIME SERIES TYPES
// ============================================

export interface UsageTrendPoint {
  timestamp: string;
  value: number;
  inputTokens: number;
  outputTokens: number;
  requests: number;
  errorRate: number;
}

export interface UsageTrend {
  data: UsageTrendPoint[];
  granularity: 'minute' | 'hour' | 'day' | 'week' | 'month';
  metric: 'tokens' | 'requests' | 'cost' | 'latency';
  analysis?: TrendAnalysis;
}

export interface TrendAnalysis {
  direction: 'increasing' | 'decreasing' | 'stable' | 'volatile';
  changeRate: number; // % change over period
  peakValue: number;
  peakTime: string;
  troughValue: number;
  troughTime: string;
  average: number;
  standardDeviation: number;
}

// ============================================
// BREAKDOWN TYPES
// ============================================

export type BreakdownDimension = 
  | 'provider' 
  | 'model' 
  | 'modelFamily' 
  | 'team' 
  | 'project' 
  | 'feature' 
  | 'environment' 
  | 'user' 
  | 'requestType'
  | 'apiKey';

export interface UsageBreakdownItem {
  dimensionValue: string;
  dimensionLabel?: string;
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  requestCount: number;
  errorCount: number;
  avgLatencyMs: number | null;
  estimatedCost: number;
  percentage: number;
  trend?: 'up' | 'down' | 'stable';
}

export interface UsageBreakdown {
  dimension: BreakdownDimension;
  items: UsageBreakdownItem[];
  total: {
    tokens: number;
    requests: number;
    cost: number;
  };
  otherCount?: number; // Items beyond limit
}

// ============================================
// USER ACTIVITY TYPES
// ============================================

export interface UserActivity {
  userIdentifier: string;
  userDisplayName?: string;
  userMetadata?: Record<string, unknown>;
  activityDate: string;
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  cachedTokens: number;
  requestCount: number;
  errorCount: number;
  estimatedCost: number;
  modelUsage: Record<string, { tokens: number; requests: number }>;
  featureUsage: Record<string, { tokens: number; requests: number }>;
  sessionCount: number;
  firstActivityAt: string;
  lastActivityAt: string;
  avgLatencyMs: number | null;
}

export interface TopUser {
  userIdentifier: string;
  userDisplayName?: string;
  totalTokens: number;
  requestCount: number;
  estimatedCost: number;
  modelUsage: Record<string, { tokens: number; requests: number }>;
  lastActivityAt: string;
  rank: number;
}

// ============================================
// QUOTA TYPES
// ============================================

export type QuotaScopeType = 'organization' | 'team' | 'project' | 'user' | 'api_key' | 'feature';
export type QuotaPeriodType = 'hourly' | 'daily' | 'weekly' | 'monthly';
export type QuotaEnforcementMode = 'alert' | 'throttle' | 'block';
export type QuotaStatus = 'ok' | 'warning' | 'critical' | 'exceeded';

export interface UsageQuota {
  id: string;
  orgId: string;
  name: string;
  description?: string;
  
  // Scope
  scopeType: QuotaScopeType;
  teamId?: string;
  projectId?: string;
  userIdentifier?: string;
  apiKeyId?: string;
  feature?: string;
  
  // Limits
  tokenLimit?: number;
  requestLimit?: number;
  inputTokenLimit?: number;
  outputTokenLimit?: number;
  
  // Period
  periodType: QuotaPeriodType;
  periodResetDay: number;
  periodResetHour: number;
  
  // Current usage
  currentTokens: number;
  currentRequests: number;
  currentInputTokens: number;
  currentOutputTokens: number;
  currentPeriodStart: string;
  currentPeriodEnd: string;
  
  // Enforcement
  enforcementMode: QuotaEnforcementMode;
  warningThresholdPct: number;
  criticalThresholdPct: number;
  
  // Status
  isActive: boolean;
  
  // Metadata
  createdBy?: string;
  createdAt: string;
  updatedAt: string;
}

export interface QuotaStatus {
  quotaId: string;
  quotaName: string;
  scopeType: QuotaScopeType;
  currentTokens: number;
  tokenLimit: number | null;
  tokenUtilizationPct: number;
  currentRequests: number;
  requestLimit: number | null;
  requestUtilizationPct: number;
  status: QuotaStatus;
  periodRemainingHours: number;
}

export interface QuotaCreateInput {
  name: string;
  description?: string;
  scopeType: QuotaScopeType;
  teamId?: string;
  projectId?: string;
  userIdentifier?: string;
  apiKeyId?: string;
  feature?: string;
  tokenLimit?: number;
  requestLimit?: number;
  inputTokenLimit?: number;
  outputTokenLimit?: number;
  periodType: QuotaPeriodType;
  periodResetDay?: number;
  periodResetHour?: number;
  enforcementMode?: QuotaEnforcementMode;
  warningThresholdPct?: number;
  criticalThresholdPct?: number;
}

// ============================================
// USAGE RECORD TYPE
// ============================================

export interface UsageRecord {
  id: string;
  orgId: string;
  providerConnectionId: string;
  provider: 'openai' | 'anthropic' | 'google' | 'azure' | 'aws';
  timestamp: string;
  bucketStart: string;
  bucketEnd: string;
  granularity: '1m' | '1h' | '1d';
  model: string;
  modelFamily?: string;
  requestType?: 'chat' | 'completion' | 'embedding' | 'image' | 'audio';
  serviceTier?: string;
  inputTokens: number;
  outputTokens: number;
  totalTokens: number;
  cachedInputTokens: number;
  cacheCreationTokens: number;
  requestCount: number;
  successCount: number;
  errorCount: number;
  totalLatencyMs: number;
  minLatencyMs?: number;
  maxLatencyMs?: number;
  p50LatencyMs?: number;
  p95LatencyMs?: number;
  p99LatencyMs?: number;
  estimatedCost: number;
  teamId?: string;
  projectId?: string;
  costCenterId?: string;
  feature?: string;
  environment: string;
  apiKeyId?: string;
  userIds: string[];
  uniqueUserCount: number;
  providerMetadata: Record<string, unknown>;
  syncId?: string;
  syncedAt: string;
  createdAt: string;
}

// ============================================
// API REQUEST/RESPONSE TYPES
// ============================================

export interface GetUsageSummaryParams {
  startDate: string;
  endDate: string;
  provider?: string;
  model?: string;
  teamId?: string;
  projectId?: string;
  feature?: string;
  environment?: string;
  comparison?: 'previous' | 'mom' | 'qoq' | 'yoy';
}

export interface GetUsageTrendParams {
  startDate: string;
  endDate: string;
  granularity?: 'minute' | 'hour' | 'day' | 'week' | 'month';
  metric?: 'tokens' | 'requests' | 'cost' | 'latency';
  provider?: string;
  model?: string;
  teamId?: string;
  projectId?: string;
}

export interface GetUsageBreakdownParams {
  startDate: string;
  endDate: string;
  dimension: BreakdownDimension;
  limit?: number;
  provider?: string;
  model?: string;
  teamId?: string;
  projectId?: string;
}

export interface GetTopUsersParams {
  startDate: string;
  endDate: string;
  limit?: number;
  orderBy?: 'tokens' | 'requests' | 'cost';
}

// ============================================
// DASHBOARD TYPES
// ============================================

export interface UsageDashboardData {
  summary: UsageSummaryWithComparison;
  tokenTrend: UsageTrend;
  requestTrend: UsageTrend;
  modelBreakdown: UsageBreakdown;
  featureBreakdown: UsageBreakdown;
  topUsers: TopUser[];
  quotaStatuses: QuotaStatus[];
  recentActivity: UsageRecord[];
}
```

---

## 4. Usage Dashboard Service

```typescript
// ============================================
// USAGE DASHBOARD SERVICE
// ============================================

import { createClient, SupabaseClient } from '@supabase/supabase-js';

export class UsageDashboardService {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // GET USAGE SUMMARY
  // ----------------------------------------
  async getUsageSummary(
    orgId: string,
    params: GetUsageSummaryParams
  ): Promise<UsageSummaryWithComparison> {
    const { startDate, endDate, provider, model, teamId, projectId, feature, environment, comparison } = params;
    
    // Get current period summary
    const { data: current, error } = await this.supabase.rpc('get_usage_summary', {
      p_org_id: orgId,
      p_start_date: startDate,
      p_end_date: endDate,
      p_provider: provider || null,
      p_model: model || null,
      p_team_id: teamId || null,
      p_project_id: projectId || null,
      p_feature: feature || null,
      p_environment: environment || null
    });
    
    if (error) throw error;
    
    const summary = this.mapToUsageSummary(current[0], startDate, endDate);
    
    // Add comparison if requested
    if (comparison) {
      const comparisonData = await this.getComparisonData(orgId, params, comparison);
      return {
        ...summary,
        comparison: comparisonData
      };
    }
    
    return summary;
  }
  
  private mapToUsageSummary(data: any, startDate: string, endDate: string): UsageSummary {
    return {
      startDate,
      endDate,
      totalTokens: Number(data.total_tokens) || 0,
      inputTokens: Number(data.input_tokens) || 0,
      outputTokens: Number(data.output_tokens) || 0,
      cachedTokens: Number(data.cached_tokens) || 0,
      cacheCreationTokens: 0,
      totalRequests: Number(data.total_requests) || 0,
      successRequests: Number(data.success_requests) || 0,
      errorRequests: Number(data.error_requests) || 0,
      errorRate: Number(data.error_rate) || 0,
      cacheHitRate: Number(data.cache_hit_rate) || 0,
      tokensPerRequest: Number(data.tokens_per_request) || 0,
      requestsPerMinute: this.calculateRequestsPerMinute(
        Number(data.total_requests) || 0,
        startDate,
        endDate
      ),
      avgLatencyMs: data.avg_latency_ms,
      minLatencyMs: null,
      maxLatencyMs: null,
      p50LatencyMs: data.p50_latency_ms,
      p95LatencyMs: data.p95_latency_ms,
      p99LatencyMs: data.p99_latency_ms,
      uniqueModels: Number(data.unique_models) || 0,
      uniqueUsers: Number(data.unique_users) || 0,
      estimatedCost: Number(data.estimated_cost) || 0
    };
  }
  
  private calculateRequestsPerMinute(totalRequests: number, startDate: string, endDate: string): number {
    const start = new Date(startDate);
    const end = new Date(endDate);
    const minutes = (end.getTime() - start.getTime()) / (1000 * 60);
    return minutes > 0 ? totalRequests / minutes : 0;
  }
  
  private async getComparisonData(
    orgId: string,
    params: GetUsageSummaryParams,
    comparisonType: 'previous' | 'mom' | 'qoq' | 'yoy'
  ): Promise<UsageSummaryWithComparison['comparison']> {
    const { previousStart, previousEnd } = this.calculateComparisonPeriod(
      params.startDate,
      params.endDate,
      comparisonType
    );
    
    const { data: previous, error } = await this.supabase.rpc('get_usage_summary', {
      p_org_id: orgId,
      p_start_date: previousStart,
      p_end_date: previousEnd,
      p_provider: params.provider || null,
      p_model: params.model || null,
      p_team_id: params.teamId || null,
      p_project_id: params.projectId || null,
      p_feature: params.feature || null,
      p_environment: params.environment || null
    });
    
    if (error) throw error;
    
    const previousSummary = this.mapToUsageSummary(previous[0], previousStart, previousEnd);
    const currentSummary = await this.getUsageSummary(orgId, { ...params, comparison: undefined });
    
    return {
      period: comparisonType,
      previousValue: previousSummary,
      changes: {
        tokens: this.calculateChange(currentSummary.totalTokens, previousSummary.totalTokens, false),
        requests: this.calculateChange(currentSummary.totalRequests, previousSummary.totalRequests, false),
        errorRate: this.calculateChange(currentSummary.errorRate, previousSummary.errorRate, true), // Lower is better
        latency: this.calculateChange(
          currentSummary.avgLatencyMs || 0,
          previousSummary.avgLatencyMs || 0,
          true // Lower is better
        ),
        cacheHitRate: this.calculateChange(currentSummary.cacheHitRate, previousSummary.cacheHitRate, false)
      }
    };
  }
  
  private calculateComparisonPeriod(
    startDate: string,
    endDate: string,
    comparisonType: 'previous' | 'mom' | 'qoq' | 'yoy'
  ): { previousStart: string; previousEnd: string } {
    const start = new Date(startDate);
    const end = new Date(endDate);
    const duration = end.getTime() - start.getTime();
    
    let previousStart: Date;
    let previousEnd: Date;
    
    switch (comparisonType) {
      case 'previous':
        previousEnd = new Date(start.getTime());
        previousStart = new Date(start.getTime() - duration);
        break;
      case 'mom':
        previousStart = new Date(start);
        previousStart.setMonth(previousStart.getMonth() - 1);
        previousEnd = new Date(end);
        previousEnd.setMonth(previousEnd.getMonth() - 1);
        break;
      case 'qoq':
        previousStart = new Date(start);
        previousStart.setMonth(previousStart.getMonth() - 3);
        previousEnd = new Date(end);
        previousEnd.setMonth(previousEnd.getMonth() - 3);
        break;
      case 'yoy':
        previousStart = new Date(start);
        previousStart.setFullYear(previousStart.getFullYear() - 1);
        previousEnd = new Date(end);
        previousEnd.setFullYear(previousEnd.getFullYear() - 1);
        break;
    }
    
    return {
      previousStart: previousStart.toISOString(),
      previousEnd: previousEnd.toISOString()
    };
  }
  
  private calculateChange(current: number, previous: number, lowerIsBetter: boolean): PercentageChange {
    const absolute = current - previous;
    const percentage = previous !== 0 ? (absolute / previous) * 100 : current > 0 ? 100 : 0;
    const direction: 'up' | 'down' | 'flat' = 
      Math.abs(percentage) < 1 ? 'flat' : percentage > 0 ? 'up' : 'down';
    
    const isImprovement = lowerIsBetter 
      ? direction === 'down' 
      : direction === 'up';
    
    return { absolute, percentage, direction, isImprovement };
  }
  
  // ----------------------------------------
  // GET FULL DASHBOARD DATA
  // ----------------------------------------
  async getDashboardData(
    orgId: string,
    params: GetUsageSummaryParams
  ): Promise<UsageDashboardData> {
    const [
      summary,
      tokenTrend,
      requestTrend,
      modelBreakdown,
      featureBreakdown,
      topUsers,
      quotaStatuses,
      recentActivity
    ] = await Promise.all([
      this.getUsageSummary(orgId, { ...params, comparison: 'previous' }),
      this.getUsageTrend(orgId, { ...params, metric: 'tokens' }),
      this.getUsageTrend(orgId, { ...params, metric: 'requests' }),
      this.getUsageBreakdown(orgId, { ...params, dimension: 'model', limit: 8 }),
      this.getUsageBreakdown(orgId, { ...params, dimension: 'feature', limit: 8 }),
      this.getTopUsers(orgId, { startDate: params.startDate, endDate: params.endDate, limit: 10 }),
      this.getQuotaStatuses(orgId),
      this.getRecentActivity(orgId, 20)
    ]);
    
    return {
      summary,
      tokenTrend,
      requestTrend,
      modelBreakdown,
      featureBreakdown,
      topUsers,
      quotaStatuses,
      recentActivity
    };
  }
  
  // ----------------------------------------
  // GET USAGE TREND
  // ----------------------------------------
  async getUsageTrend(
    orgId: string,
    params: GetUsageTrendParams
  ): Promise<UsageTrend> {
    const granularity = params.granularity || this.inferGranularity(params.startDate, params.endDate);
    const metric = params.metric || 'tokens';
    
    const { data, error } = await this.supabase.rpc('get_usage_trend', {
      p_org_id: orgId,
      p_start_date: params.startDate,
      p_end_date: params.endDate,
      p_granularity: granularity,
      p_metric: metric
    });
    
    if (error) throw error;
    
    const trendData: UsageTrendPoint[] = (data || []).map((row: any) => ({
      timestamp: row.time_bucket,
      value: Number(row.value),
      inputTokens: Number(row.input_tokens),
      outputTokens: Number(row.output_tokens),
      requests: Number(row.requests),
      errorRate: Number(row.error_rate)
    }));
    
    return {
      data: trendData,
      granularity,
      metric,
      analysis: this.analyzeTrend(trendData)
    };
  }
  
  private inferGranularity(startDate: string, endDate: string): 'minute' | 'hour' | 'day' | 'week' | 'month' {
    const start = new Date(startDate);
    const end = new Date(endDate);
    const days = (end.getTime() - start.getTime()) / (1000 * 60 * 60 * 24);
    
    if (days <= 1) return 'minute';
    if (days <= 7) return 'hour';
    if (days <= 90) return 'day';
    if (days <= 365) return 'week';
    return 'month';
  }
  
  private analyzeTrend(data: UsageTrendPoint[]): TrendAnalysis {
    if (data.length === 0) {
      return {
        direction: 'stable',
        changeRate: 0,
        peakValue: 0,
        peakTime: '',
        troughValue: 0,
        troughTime: '',
        average: 0,
        standardDeviation: 0
      };
    }
    
    const values = data.map(d => d.value);
    const average = values.reduce((a, b) => a + b, 0) / values.length;
    const variance = values.reduce((sum, val) => sum + Math.pow(val - average, 2), 0) / values.length;
    const standardDeviation = Math.sqrt(variance);
    
    // Find peak and trough
    let peakIdx = 0, troughIdx = 0;
    for (let i = 1; i < values.length; i++) {
      if (values[i] > values[peakIdx]) peakIdx = i;
      if (values[i] < values[troughIdx]) troughIdx = i;
    }
    
    // Calculate trend direction using linear regression
    const n = values.length;
    let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
    for (let i = 0; i < n; i++) {
      sumX += i;
      sumY += values[i];
      sumXY += i * values[i];
      sumX2 += i * i;
    }
    const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
    
    // Calculate change rate
    const firstValue = values[0] || 1;
    const lastValue = values[values.length - 1] || 0;
    const changeRate = ((lastValue - firstValue) / firstValue) * 100;
    
    // Determine direction
    let direction: 'increasing' | 'decreasing' | 'stable' | 'volatile';
    const cv = standardDeviation / average; // Coefficient of variation
    
    if (cv > 0.5) {
      direction = 'volatile';
    } else if (Math.abs(slope) < average * 0.01) {
      direction = 'stable';
    } else {
      direction = slope > 0 ? 'increasing' : 'decreasing';
    }
    
    return {
      direction,
      changeRate,
      peakValue: values[peakIdx],
      peakTime: data[peakIdx].timestamp,
      troughValue: values[troughIdx],
      troughTime: data[troughIdx].timestamp,
      average,
      standardDeviation
    };
  }
  
  // ----------------------------------------
  // GET USAGE BREAKDOWN
  // ----------------------------------------
  async getUsageBreakdown(
    orgId: string,
    params: GetUsageBreakdownParams
  ): Promise<UsageBreakdown> {
    const { startDate, endDate, dimension, limit = 10 } = params;
    
    // Map dimension to database column
    const dbDimension = this.mapDimensionToColumn(dimension);
    
    const { data, error } = await this.supabase.rpc('get_usage_breakdown', {
      p_org_id: orgId,
      p_start_date: startDate,
      p_end_date: endDate,
      p_dimension: dbDimension,
      p_limit: limit
    });
    
    if (error) throw error;
    
    const items: UsageBreakdownItem[] = (data || []).map((row: any) => ({
      dimensionValue: row.dimension_value,
      dimensionLabel: this.getDimensionLabel(dimension, row.dimension_value),
      totalTokens: Number(row.total_tokens),
      inputTokens: Number(row.input_tokens),
      outputTokens: Number(row.output_tokens),
      requestCount: Number(row.request_count),
      errorCount: Number(row.error_count),
      avgLatencyMs: row.avg_latency_ms,
      estimatedCost: Number(row.estimated_cost),
      percentage: Number(row.percentage)
    }));
    
    const total = {
      tokens: items.reduce((sum, item) => sum + item.totalTokens, 0),
      requests: items.reduce((sum, item) => sum + item.requestCount, 0),
      cost: items.reduce((sum, item) => sum + item.estimatedCost, 0)
    };
    
    return { dimension, items, total };
  }
  
  private mapDimensionToColumn(dimension: BreakdownDimension): string {
    const mapping: Record<BreakdownDimension, string> = {
      provider: 'provider',
      model: 'model',
      modelFamily: 'model_family',
      team: 'team_id',
      project: 'project_id',
      feature: 'feature',
      environment: 'environment',
      user: 'user_identifier',
      requestType: 'request_type',
      apiKey: 'api_key_id'
    };
    return mapping[dimension] || dimension;
  }
  
  private getDimensionLabel(dimension: BreakdownDimension, value: string): string {
    // Could fetch labels from related tables (teams, projects, etc.)
    // For now, return the value as-is
    return value;
  }
  
  // ----------------------------------------
  // GET TOP USERS
  // ----------------------------------------
  async getTopUsers(
    orgId: string,
    params: GetTopUsersParams
  ): Promise<TopUser[]> {
    const { startDate, endDate, limit = 10, orderBy = 'tokens' } = params;
    
    const { data, error } = await this.supabase.rpc('get_top_users', {
      p_org_id: orgId,
      p_start_date: startDate,
      p_end_date: endDate,
      p_limit: limit,
      p_order_by: orderBy
    });
    
    if (error) throw error;
    
    return (data || []).map((row: any, index: number) => ({
      userIdentifier: row.user_identifier,
      userDisplayName: row.user_display_name,
      totalTokens: Number(row.total_tokens),
      requestCount: Number(row.request_count),
      estimatedCost: Number(row.estimated_cost),
      modelUsage: row.model_usage || {},
      lastActivityAt: row.last_activity_at,
      rank: index + 1
    }));
  }
  
  // ----------------------------------------
  // GET QUOTA STATUSES
  // ----------------------------------------
  async getQuotaStatuses(orgId: string): Promise<QuotaStatus[]> {
    const { data: quotas, error: quotaError } = await this.supabase
      .from('usage_quotas')
      .select('id')
      .eq('org_id', orgId)
      .eq('is_active', true);
    
    if (quotaError) throw quotaError;
    
    const statuses: QuotaStatus[] = [];
    
    for (const quota of quotas || []) {
      const { data, error } = await this.supabase.rpc('check_quota_status', {
        p_quota_id: quota.id
      });
      
      if (!error && data && data[0]) {
        statuses.push({
          quotaId: data[0].quota_id,
          quotaName: data[0].quota_name,
          scopeType: data[0].scope_type,
          currentTokens: Number(data[0].current_tokens),
          tokenLimit: data[0].token_limit,
          tokenUtilizationPct: Number(data[0].token_utilization_pct),
          currentRequests: Number(data[0].current_requests),
          requestLimit: data[0].request_limit,
          requestUtilizationPct: Number(data[0].request_utilization_pct),
          status: data[0].status,
          periodRemainingHours: Number(data[0].period_remaining_hours)
        });
      }
    }
    
    return statuses;
  }
  
  // ----------------------------------------
  // GET RECENT ACTIVITY
  // ----------------------------------------
  async getRecentActivity(orgId: string, limit: number = 20): Promise<UsageRecord[]> {
    const { data, error } = await this.supabase
      .from('usage_records')
      .select('*')
      .eq('org_id', orgId)
      .order('timestamp', { ascending: false })
      .limit(limit);
    
    if (error) throw error;
    
    return (data || []).map(this.mapToUsageRecord);
  }
  
  private mapToUsageRecord(row: any): UsageRecord {
    return {
      id: row.id,
      orgId: row.org_id,
      providerConnectionId: row.provider_connection_id,
      provider: row.provider,
      timestamp: row.timestamp,
      bucketStart: row.bucket_start,
      bucketEnd: row.bucket_end,
      granularity: row.granularity,
      model: row.model,
      modelFamily: row.model_family,
      requestType: row.request_type,
      serviceTier: row.service_tier,
      inputTokens: Number(row.input_tokens),
      outputTokens: Number(row.output_tokens),
      totalTokens: Number(row.total_tokens),
      cachedInputTokens: Number(row.cached_input_tokens),
      cacheCreationTokens: Number(row.cache_creation_tokens),
      requestCount: Number(row.request_count),
      successCount: Number(row.success_count),
      errorCount: Number(row.error_count),
      totalLatencyMs: Number(row.total_latency_ms),
      minLatencyMs: row.min_latency_ms,
      maxLatencyMs: row.max_latency_ms,
      p50LatencyMs: row.p50_latency_ms,
      p95LatencyMs: row.p95_latency_ms,
      p99LatencyMs: row.p99_latency_ms,
      estimatedCost: Number(row.estimated_cost),
      teamId: row.team_id,
      projectId: row.project_id,
      costCenterId: row.cost_center_id,
      feature: row.feature,
      environment: row.environment,
      apiKeyId: row.api_key_id,
      userIds: row.user_ids || [],
      uniqueUserCount: Number(row.unique_user_count),
      providerMetadata: row.provider_metadata || {},
      syncId: row.sync_id,
      syncedAt: row.synced_at,
      createdAt: row.created_at
    };
  }
}

export const usageDashboardService = new UsageDashboardService();
```

---

## 5. Usage Analytics Engine

```typescript
// ============================================
// USAGE ANALYTICS ENGINE
// ============================================

export class UsageAnalyticsEngine {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // TOKEN EFFICIENCY ANALYSIS
  // ----------------------------------------
  async analyzeTokenEfficiency(
    orgId: string,
    startDate: string,
    endDate: string
  ): Promise<TokenEfficiencyAnalysis> {
    const { data, error } = await this.supabase
      .from('hourly_usage_rollup')
      .select('*')
      .eq('org_id', orgId)
      .gte('hour', startDate)
      .lt('hour', endDate);
    
    if (error) throw error;
    
    const rows = data || [];
    
    // Calculate efficiency metrics
    const totalInput = rows.reduce((sum, r) => sum + Number(r.input_tokens), 0);
    const totalOutput = rows.reduce((sum, r) => sum + Number(r.output_tokens), 0);
    const totalCached = rows.reduce((sum, r) => sum + Number(r.cached_input_tokens), 0);
    const totalRequests = rows.reduce((sum, r) => sum + Number(r.request_count), 0);
    
    // Input/Output ratio (lower is more efficient for many use cases)
    const inputOutputRatio = totalOutput > 0 ? totalInput / totalOutput : 0;
    
    // Cache efficiency
    const cacheHitRate = totalInput > 0 ? (totalCached / totalInput) * 100 : 0;
    
    // Tokens per request (lower can indicate better prompt engineering)
    const avgTokensPerRequest = totalRequests > 0 ? (totalInput + totalOutput) / totalRequests : 0;
    
    // Model efficiency breakdown
    const modelEfficiency = this.calculateModelEfficiency(rows);
    
    // Feature efficiency breakdown
    const featureEfficiency = this.calculateFeatureEfficiency(rows);
    
    // Recommendations
    const recommendations = this.generateEfficiencyRecommendations({
      inputOutputRatio,
      cacheHitRate,
      avgTokensPerRequest,
      modelEfficiency,
      featureEfficiency
    });
    
    return {
      period: { startDate, endDate },
      metrics: {
        totalInputTokens: totalInput,
        totalOutputTokens: totalOutput,
        totalCachedTokens: totalCached,
        totalRequests,
        inputOutputRatio,
        cacheHitRate,
        avgTokensPerRequest
      },
      modelEfficiency,
      featureEfficiency,
      recommendations
    };
  }
  
  private calculateModelEfficiency(rows: any[]): ModelEfficiencyMetric[] {
    const byModel = new Map<string, any>();
    
    for (const row of rows) {
      const model = row.model || 'unknown';
      const existing = byModel.get(model) || {
        inputTokens: 0,
        outputTokens: 0,
        cachedTokens: 0,
        requests: 0,
        cost: 0
      };
      
      byModel.set(model, {
        inputTokens: existing.inputTokens + Number(row.input_tokens),
        outputTokens: existing.outputTokens + Number(row.output_tokens),
        cachedTokens: existing.cachedTokens + Number(row.cached_input_tokens),
        requests: existing.requests + Number(row.request_count),
        cost: existing.cost + Number(row.estimated_cost)
      });
    }
    
    return Array.from(byModel.entries()).map(([model, data]) => ({
      model,
      inputTokens: data.inputTokens,
      outputTokens: data.outputTokens,
      tokensPerRequest: data.requests > 0 ? (data.inputTokens + data.outputTokens) / data.requests : 0,
      cacheHitRate: data.inputTokens > 0 ? (data.cachedTokens / data.inputTokens) * 100 : 0,
      costPerRequest: data.requests > 0 ? data.cost / data.requests : 0,
      costPer1kTokens: (data.inputTokens + data.outputTokens) > 0 
        ? (data.cost / (data.inputTokens + data.outputTokens)) * 1000 
        : 0
    })).sort((a, b) => b.inputTokens + b.outputTokens - (a.inputTokens + a.outputTokens));
  }
  
  private calculateFeatureEfficiency(rows: any[]): FeatureEfficiencyMetric[] {
    const byFeature = new Map<string, any>();
    
    for (const row of rows) {
      const feature = row.feature || 'untagged';
      const existing = byFeature.get(feature) || {
        inputTokens: 0,
        outputTokens: 0,
        requests: 0,
        cost: 0,
        errors: 0
      };
      
      byFeature.set(feature, {
        inputTokens: existing.inputTokens + Number(row.input_tokens),
        outputTokens: existing.outputTokens + Number(row.output_tokens),
        requests: existing.requests + Number(row.request_count),
        cost: existing.cost + Number(row.estimated_cost),
        errors: existing.errors + Number(row.error_count)
      });
    }
    
    return Array.from(byFeature.entries()).map(([feature, data]) => ({
      feature,
      totalTokens: data.inputTokens + data.outputTokens,
      requestCount: data.requests,
      tokensPerRequest: data.requests > 0 ? (data.inputTokens + data.outputTokens) / data.requests : 0,
      errorRate: data.requests > 0 ? (data.errors / data.requests) * 100 : 0,
      estimatedCost: data.cost
    })).sort((a, b) => b.totalTokens - a.totalTokens);
  }
  
  private generateEfficiencyRecommendations(metrics: any): EfficiencyRecommendation[] {
    const recommendations: EfficiencyRecommendation[] = [];
    
    // High input/output ratio might indicate verbose prompts
    if (metrics.inputOutputRatio > 5) {
      recommendations.push({
        type: 'prompt_optimization',
        priority: 'high',
        title: 'High Input/Output Ratio',
        description: `Your input/output token ratio is ${metrics.inputOutputRatio.toFixed(1)}:1. Consider optimizing prompts to reduce input tokens.`,
        potentialSavings: '15-30%',
        action: 'Review system prompts and reduce unnecessary context'
      });
    }
    
    // Low cache hit rate
    if (metrics.cacheHitRate < 20) {
      recommendations.push({
        type: 'caching',
        priority: 'medium',
        title: 'Low Cache Hit Rate',
        description: `Cache hit rate is only ${metrics.cacheHitRate.toFixed(1)}%. Implementing prompt caching could reduce costs.`,
        potentialSavings: '20-40%',
        action: 'Enable semantic caching for repeated queries'
      });
    }
    
    // High tokens per request
    if (metrics.avgTokensPerRequest > 10000) {
      recommendations.push({
        type: 'request_optimization',
        priority: 'medium',
        title: 'High Tokens Per Request',
        description: `Average ${metrics.avgTokensPerRequest.toFixed(0)} tokens per request. Consider chunking or summarizing content.`,
        potentialSavings: '10-25%',
        action: 'Implement content chunking or pre-summarization'
      });
    }
    
    // Model-specific recommendations
    for (const model of metrics.modelEfficiency) {
      if (model.costPer1kTokens > 0.05 && model.tokensPerRequest < 1000) {
        recommendations.push({
          type: 'model_selection',
          priority: 'high',
          title: `Consider Smaller Model for ${model.model}`,
          description: `Using expensive model for small requests (${model.tokensPerRequest.toFixed(0)} tokens/request).`,
          potentialSavings: '30-60%',
          action: `Route small requests to a lighter model`
        });
      }
    }
    
    return recommendations.sort((a, b) => {
      const priorityOrder = { high: 0, medium: 1, low: 2 };
      return priorityOrder[a.priority] - priorityOrder[b.priority];
    });
  }
  
  // ----------------------------------------
  // USAGE PATTERN ANALYSIS
  // ----------------------------------------
  async analyzeUsagePatterns(
    orgId: string,
    startDate: string,
    endDate: string
  ): Promise<UsagePatternAnalysis> {
    const { data, error } = await this.supabase
      .from('hourly_usage_rollup')
      .select('*')
      .eq('org_id', orgId)
      .gte('hour', startDate)
      .lt('hour', endDate)
      .order('hour', { ascending: true });
    
    if (error) throw error;
    
    const rows = data || [];
    
    // Hour of day patterns
    const hourlyPattern = this.calculateHourlyPattern(rows);
    
    // Day of week patterns
    const dailyPattern = this.calculateDailyPattern(rows);
    
    // Peak usage analysis
    const peakAnalysis = this.analyzePeakUsage(rows);
    
    // Growth analysis
    const growthAnalysis = this.analyzeGrowth(rows);
    
    return {
      period: { startDate, endDate },
      hourlyPattern,
      dailyPattern,
      peakAnalysis,
      growthAnalysis
    };
  }
  
  private calculateHourlyPattern(rows: any[]): HourlyPattern[] {
    const byHour = new Map<number, { tokens: number; requests: number; count: number }>();
    
    for (const row of rows) {
      const hour = new Date(row.hour).getUTCHours();
      const existing = byHour.get(hour) || { tokens: 0, requests: 0, count: 0 };
      byHour.set(hour, {
        tokens: existing.tokens + Number(row.total_tokens),
        requests: existing.requests + Number(row.request_count),
        count: existing.count + 1
      });
    }
    
    return Array.from({ length: 24 }, (_, hour) => {
      const data = byHour.get(hour) || { tokens: 0, requests: 0, count: 1 };
      return {
        hour,
        avgTokens: data.tokens / data.count,
        avgRequests: data.requests / data.count,
        relativeActivity: 0 // Will be normalized below
      };
    }).map((item, _, arr) => {
      const maxTokens = Math.max(...arr.map(a => a.avgTokens));
      return {
        ...item,
        relativeActivity: maxTokens > 0 ? (item.avgTokens / maxTokens) * 100 : 0
      };
    });
  }
  
  private calculateDailyPattern(rows: any[]): DailyPattern[] {
    const byDay = new Map<number, { tokens: number; requests: number; count: number }>();
    
    for (const row of rows) {
      const day = new Date(row.hour).getUTCDay();
      const existing = byDay.get(day) || { tokens: 0, requests: 0, count: 0 };
      byDay.set(day, {
        tokens: existing.tokens + Number(row.total_tokens),
        requests: existing.requests + Number(row.request_count),
        count: existing.count + 1
      });
    }
    
    const dayNames = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'];
    
    return Array.from({ length: 7 }, (_, day) => {
      const data = byDay.get(day) || { tokens: 0, requests: 0, count: 1 };
      return {
        day,
        dayName: dayNames[day],
        avgTokens: data.tokens / Math.max(data.count, 1),
        avgRequests: data.requests / Math.max(data.count, 1)
      };
    });
  }
  
  private analyzePeakUsage(rows: any[]): PeakAnalysis {
    if (rows.length === 0) {
      return {
        peakHour: null,
        peakDay: null,
        peakTokens: 0,
        peakRequests: 0,
        averageTokens: 0,
        peakToAverageRatio: 0
      };
    }
    
    let peakRow = rows[0];
    let totalTokens = 0;
    
    for (const row of rows) {
      const tokens = Number(row.total_tokens);
      totalTokens += tokens;
      if (tokens > Number(peakRow.total_tokens)) {
        peakRow = row;
      }
    }
    
    const averageTokens = totalTokens / rows.length;
    const peakTokens = Number(peakRow.total_tokens);
    
    return {
      peakHour: peakRow.hour,
      peakDay: new Date(peakRow.hour).toLocaleDateString('en-US', { weekday: 'long' }),
      peakTokens,
      peakRequests: Number(peakRow.request_count),
      averageTokens,
      peakToAverageRatio: averageTokens > 0 ? peakTokens / averageTokens : 0
    };
  }
  
  private analyzeGrowth(rows: any[]): GrowthAnalysis {
    if (rows.length < 2) {
      return {
        tokenGrowthRate: 0,
        requestGrowthRate: 0,
        trend: 'stable',
        projectedMonthlyTokens: 0
      };
    }
    
    // Split into first half and second half
    const midpoint = Math.floor(rows.length / 2);
    const firstHalf = rows.slice(0, midpoint);
    const secondHalf = rows.slice(midpoint);
    
    const firstHalfTokens = firstHalf.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    const secondHalfTokens = secondHalf.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    const firstHalfRequests = firstHalf.reduce((sum, r) => sum + Number(r.request_count), 0);
    const secondHalfRequests = secondHalf.reduce((sum, r) => sum + Number(r.request_count), 0);
    
    const tokenGrowthRate = firstHalfTokens > 0 
      ? ((secondHalfTokens - firstHalfTokens) / firstHalfTokens) * 100 
      : 0;
    const requestGrowthRate = firstHalfRequests > 0 
      ? ((secondHalfRequests - firstHalfRequests) / firstHalfRequests) * 100 
      : 0;
    
    // Determine trend
    let trend: 'increasing' | 'decreasing' | 'stable';
    if (Math.abs(tokenGrowthRate) < 5) {
      trend = 'stable';
    } else {
      trend = tokenGrowthRate > 0 ? 'increasing' : 'decreasing';
    }
    
    // Project monthly usage
    const totalTokens = rows.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    const periodHours = rows.length;
    const hourlyAverage = totalTokens / periodHours;
    const projectedMonthlyTokens = hourlyAverage * 24 * 30;
    
    return {
      tokenGrowthRate,
      requestGrowthRate,
      trend,
      projectedMonthlyTokens
    };
  }
  
  // ----------------------------------------
  // MODEL USAGE ANALYSIS
  // ----------------------------------------
  async analyzeModelUsage(
    orgId: string,
    startDate: string,
    endDate: string
  ): Promise<ModelUsageAnalysis> {
    const { data, error } = await this.supabase
      .from('daily_usage_rollup')
      .select('*')
      .eq('org_id', orgId)
      .gte('date', startDate)
      .lt('date', endDate);
    
    if (error) throw error;
    
    const rows = data || [];
    
    // Group by model
    const byModel = new Map<string, ModelStats>();
    
    for (const row of rows) {
      const model = row.model || 'unknown';
      const existing = byModel.get(model) || {
        model,
        provider: row.provider,
        modelFamily: row.model_family,
        totalTokens: 0,
        inputTokens: 0,
        outputTokens: 0,
        requestCount: 0,
        errorCount: 0,
        estimatedCost: 0,
        firstUsed: row.date,
        lastUsed: row.date,
        dailyUsage: []
      };
      
      byModel.set(model, {
        ...existing,
        totalTokens: existing.totalTokens + Number(row.total_tokens),
        inputTokens: existing.inputTokens + Number(row.input_tokens),
        outputTokens: existing.outputTokens + Number(row.output_tokens),
        requestCount: existing.requestCount + Number(row.request_count),
        errorCount: existing.errorCount + Number(row.error_count),
        estimatedCost: existing.estimatedCost + Number(row.estimated_cost),
        lastUsed: row.date > existing.lastUsed ? row.date : existing.lastUsed,
        dailyUsage: [...existing.dailyUsage, { date: row.date, tokens: Number(row.total_tokens) }]
      });
    }
    
    const models = Array.from(byModel.values())
      .sort((a, b) => b.totalTokens - a.totalTokens);
    
    // Calculate distribution
    const totalTokens = models.reduce((sum, m) => sum + m.totalTokens, 0);
    const distribution = models.map(m => ({
      model: m.model,
      percentage: totalTokens > 0 ? (m.totalTokens / totalTokens) * 100 : 0
    }));
    
    // Identify trends
    const trends = models.map(m => {
      const usageByDate = m.dailyUsage.sort((a, b) => a.date.localeCompare(b.date));
      if (usageByDate.length < 2) return { model: m.model, trend: 'stable' as const };
      
      const firstHalf = usageByDate.slice(0, Math.floor(usageByDate.length / 2));
      const secondHalf = usageByDate.slice(Math.floor(usageByDate.length / 2));
      
      const firstAvg = firstHalf.reduce((sum, d) => sum + d.tokens, 0) / firstHalf.length;
      const secondAvg = secondHalf.reduce((sum, d) => sum + d.tokens, 0) / secondHalf.length;
      
      const change = firstAvg > 0 ? ((secondAvg - firstAvg) / firstAvg) * 100 : 0;
      
      return {
        model: m.model,
        trend: Math.abs(change) < 10 ? 'stable' as const : change > 0 ? 'increasing' as const : 'decreasing' as const,
        changePercentage: change
      };
    });
    
    return {
      period: { startDate, endDate },
      models,
      distribution,
      trends,
      totalModelsUsed: models.length,
      primaryModel: models[0]?.model || null
    };
  }
}

// ============================================
// TYPES FOR ANALYTICS ENGINE
// ============================================

export interface TokenEfficiencyAnalysis {
  period: { startDate: string; endDate: string };
  metrics: {
    totalInputTokens: number;
    totalOutputTokens: number;
    totalCachedTokens: number;
    totalRequests: number;
    inputOutputRatio: number;
    cacheHitRate: number;
    avgTokensPerRequest: number;
  };
  modelEfficiency: ModelEfficiencyMetric[];
  featureEfficiency: FeatureEfficiencyMetric[];
  recommendations: EfficiencyRecommendation[];
}

export interface ModelEfficiencyMetric {
  model: string;
  inputTokens: number;
  outputTokens: number;
  tokensPerRequest: number;
  cacheHitRate: number;
  costPerRequest: number;
  costPer1kTokens: number;
}

export interface FeatureEfficiencyMetric {
  feature: string;
  totalTokens: number;
  requestCount: number;
  tokensPerRequest: number;
  errorRate: number;
  estimatedCost: number;
}

export interface EfficiencyRecommendation {
  type: 'prompt_optimization' | 'caching' | 'model_selection' | 'request_optimization';
  priority: 'high' | 'medium' | 'low';
  title: string;
  description: string;
  potentialSavings: string;
  action: string;
}

export interface UsagePatternAnalysis {
  period: { startDate: string; endDate: string };
  hourlyPattern: HourlyPattern[];
  dailyPattern: DailyPattern[];
  peakAnalysis: PeakAnalysis;
  growthAnalysis: GrowthAnalysis;
}

export interface HourlyPattern {
  hour: number;
  avgTokens: number;
  avgRequests: number;
  relativeActivity: number;
}

export interface DailyPattern {
  day: number;
  dayName: string;
  avgTokens: number;
  avgRequests: number;
}

export interface PeakAnalysis {
  peakHour: string | null;
  peakDay: string | null;
  peakTokens: number;
  peakRequests: number;
  averageTokens: number;
  peakToAverageRatio: number;
}

export interface GrowthAnalysis {
  tokenGrowthRate: number;
  requestGrowthRate: number;
  trend: 'increasing' | 'decreasing' | 'stable';
  projectedMonthlyTokens: number;
}

export interface ModelUsageAnalysis {
  period: { startDate: string; endDate: string };
  models: ModelStats[];
  distribution: { model: string; percentage: number }[];
  trends: { model: string; trend: 'increasing' | 'decreasing' | 'stable'; changePercentage?: number }[];
  totalModelsUsed: number;
  primaryModel: string | null;
}

export interface ModelStats {
  model: string;
  provider: string;
  modelFamily: string;
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  requestCount: number;
  errorCount: number;
  estimatedCost: number;
  firstUsed: string;
  lastUsed: string;
  dailyUsage: { date: string; tokens: number }[];
}

export const usageAnalyticsEngine = new UsageAnalyticsEngine();
```

---

## 6. Usage Breakdown Service

```typescript
// ============================================
// USAGE BREAKDOWN SERVICE
// ============================================

export class UsageBreakdownService {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // MULTI-DIMENSIONAL BREAKDOWN
  // ----------------------------------------
  async getMultiDimensionalBreakdown(
    orgId: string,
    startDate: string,
    endDate: string,
    dimensions: BreakdownDimension[],
    filters?: Record<string, string>
  ): Promise<MultiDimensionalBreakdown> {
    const breakdowns: Record<string, UsageBreakdown> = {};
    
    for (const dimension of dimensions) {
      breakdowns[dimension] = await usageDashboardService.getUsageBreakdown(orgId, {
        startDate,
        endDate,
        dimension,
        limit: 10,
        ...filters
      });
    }
    
    // Calculate cross-dimensional insights
    const crossDimensionalInsights = this.calculateCrossDimensionalInsights(breakdowns);
    
    return {
      period: { startDate, endDate },
      dimensions,
      breakdowns,
      insights: crossDimensionalInsights
    };
  }
  
  private calculateCrossDimensionalInsights(
    breakdowns: Record<string, UsageBreakdown>
  ): CrossDimensionalInsight[] {
    const insights: CrossDimensionalInsight[] = [];
    
    // Model-Feature correlation
    if (breakdowns.model && breakdowns.feature) {
      const topModel = breakdowns.model.items[0];
      const topFeature = breakdowns.feature.items[0];
      
      if (topModel && topFeature) {
        insights.push({
          type: 'concentration',
          title: 'Usage Concentration',
          description: `${topModel.dimensionValue} handles ${topModel.percentage.toFixed(1)}% of tokens, primarily from ${topFeature.dimensionValue} feature`,
          severity: topModel.percentage > 70 ? 'warning' : 'info'
        });
      }
    }
    
    // Provider diversification
    if (breakdowns.provider) {
      const providerCount = breakdowns.provider.items.length;
      if (providerCount === 1) {
        insights.push({
          type: 'diversification',
          title: 'Single Provider Dependency',
          description: 'All usage is concentrated on a single provider. Consider diversification for resilience.',
          severity: 'warning'
        });
      }
    }
    
    // Environment breakdown
    if (breakdowns.environment) {
      const prodUsage = breakdowns.environment.items.find(i => 
        i.dimensionValue === 'production' || i.dimensionValue === 'prod'
      );
      const nonProdUsage = breakdowns.environment.items.filter(i => 
        i.dimensionValue !== 'production' && i.dimensionValue !== 'prod'
      );
      
      const nonProdTotal = nonProdUsage.reduce((sum, i) => sum + i.totalTokens, 0);
      const prodTotal = prodUsage?.totalTokens || 0;
      
      if (nonProdTotal > prodTotal * 0.3) {
        insights.push({
          type: 'efficiency',
          title: 'High Non-Production Usage',
          description: `Non-production environments account for ${((nonProdTotal / (prodTotal + nonProdTotal)) * 100).toFixed(1)}% of total usage`,
          severity: 'info'
        });
      }
    }
    
    return insights;
  }
  
  // ----------------------------------------
  // DRILLDOWN BREAKDOWN
  // ----------------------------------------
  async getDrilldownBreakdown(
    orgId: string,
    startDate: string,
    endDate: string,
    primaryDimension: BreakdownDimension,
    primaryValue: string,
    secondaryDimension: BreakdownDimension
  ): Promise<DrilldownBreakdown> {
    // Get the secondary breakdown filtered by primary dimension
    const { data, error } = await this.supabase
      .from('usage_records')
      .select(`
        ${this.mapDimensionToColumn(secondaryDimension)},
        input_tokens,
        output_tokens,
        total_tokens,
        request_count,
        error_count,
        estimated_cost
      `)
      .eq('org_id', orgId)
      .eq(this.mapDimensionToColumn(primaryDimension), primaryValue)
      .gte('timestamp', startDate)
      .lt('timestamp', endDate);
    
    if (error) throw error;
    
    // Aggregate by secondary dimension
    const aggregated = new Map<string, any>();
    
    for (const row of data || []) {
      const key = row[this.mapDimensionToColumn(secondaryDimension)] || 'Unknown';
      const existing = aggregated.get(key) || {
        totalTokens: 0,
        inputTokens: 0,
        outputTokens: 0,
        requestCount: 0,
        errorCount: 0,
        estimatedCost: 0
      };
      
      aggregated.set(key, {
        totalTokens: existing.totalTokens + Number(row.total_tokens),
        inputTokens: existing.inputTokens + Number(row.input_tokens),
        outputTokens: existing.outputTokens + Number(row.output_tokens),
        requestCount: existing.requestCount + Number(row.request_count),
        errorCount: existing.errorCount + Number(row.error_count),
        estimatedCost: existing.estimatedCost + Number(row.estimated_cost)
      });
    }
    
    const totalTokens = Array.from(aggregated.values()).reduce((sum, v) => sum + v.totalTokens, 0);
    
    const items: UsageBreakdownItem[] = Array.from(aggregated.entries())
      .map(([key, value]) => ({
        dimensionValue: key,
        totalTokens: value.totalTokens,
        inputTokens: value.inputTokens,
        outputTokens: value.outputTokens,
        requestCount: value.requestCount,
        errorCount: value.errorCount,
        avgLatencyMs: null,
        estimatedCost: value.estimatedCost,
        percentage: totalTokens > 0 ? (value.totalTokens / totalTokens) * 100 : 0
      }))
      .sort((a, b) => b.totalTokens - a.totalTokens);
    
    return {
      primaryDimension,
      primaryValue,
      secondaryDimension,
      items,
      total: {
        tokens: totalTokens,
        requests: items.reduce((sum, i) => sum + i.requestCount, 0),
        cost: items.reduce((sum, i) => sum + i.estimatedCost, 0)
      }
    };
  }
  
  private mapDimensionToColumn(dimension: BreakdownDimension): string {
    const mapping: Record<BreakdownDimension, string> = {
      provider: 'provider',
      model: 'model',
      modelFamily: 'model_family',
      team: 'team_id',
      project: 'project_id',
      feature: 'feature',
      environment: 'environment',
      user: 'user_identifier',
      requestType: 'request_type',
      apiKey: 'api_key_id'
    };
    return mapping[dimension] || dimension;
  }
  
  // ----------------------------------------
  // COMPARATIVE BREAKDOWN
  // ----------------------------------------
  async getComparativeBreakdown(
    orgId: string,
    currentStart: string,
    currentEnd: string,
    previousStart: string,
    previousEnd: string,
    dimension: BreakdownDimension
  ): Promise<ComparativeBreakdown> {
    const [current, previous] = await Promise.all([
      usageDashboardService.getUsageBreakdown(orgId, { 
        startDate: currentStart, 
        endDate: currentEnd, 
        dimension 
      }),
      usageDashboardService.getUsageBreakdown(orgId, { 
        startDate: previousStart, 
        endDate: previousEnd, 
        dimension 
      })
    ]);
    
    // Create lookup for previous values
    const previousLookup = new Map(previous.items.map(i => [i.dimensionValue, i]));
    
    // Calculate changes
    const comparisons: ComparisonItem[] = current.items.map(item => {
      const prev = previousLookup.get(item.dimensionValue);
      
      return {
        dimensionValue: item.dimensionValue,
        current: item,
        previous: prev || null,
        tokenChange: prev ? item.totalTokens - prev.totalTokens : item.totalTokens,
        tokenChangePercent: prev && prev.totalTokens > 0 
          ? ((item.totalTokens - prev.totalTokens) / prev.totalTokens) * 100 
          : 100,
        requestChange: prev ? item.requestCount - prev.requestCount : item.requestCount,
        isNew: !prev
      };
    });
    
    // Find churned items (in previous but not current)
    const currentKeys = new Set(current.items.map(i => i.dimensionValue));
    const churned = previous.items
      .filter(i => !currentKeys.has(i.dimensionValue))
      .map(i => ({
        dimensionValue: i.dimensionValue,
        current: null,
        previous: i,
        tokenChange: -i.totalTokens,
        tokenChangePercent: -100,
        requestChange: -i.requestCount,
        isChurned: true
      }));
    
    return {
      dimension,
      currentPeriod: { start: currentStart, end: currentEnd },
      previousPeriod: { start: previousStart, end: previousEnd },
      comparisons: [...comparisons, ...churned].sort((a, b) => 
        Math.abs(b.tokenChange) - Math.abs(a.tokenChange)
      ),
      summary: {
        totalGrowth: current.total.tokens - previous.total.tokens,
        growthPercent: previous.total.tokens > 0 
          ? ((current.total.tokens - previous.total.tokens) / previous.total.tokens) * 100 
          : 100,
        newItems: comparisons.filter(c => c.isNew).length,
        churnedItems: churned.length
      }
    };
  }
}

// ============================================
// BREAKDOWN TYPES
// ============================================

export interface MultiDimensionalBreakdown {
  period: { startDate: string; endDate: string };
  dimensions: BreakdownDimension[];
  breakdowns: Record<string, UsageBreakdown>;
  insights: CrossDimensionalInsight[];
}

export interface CrossDimensionalInsight {
  type: 'concentration' | 'diversification' | 'efficiency' | 'anomaly';
  title: string;
  description: string;
  severity: 'info' | 'warning' | 'critical';
}

export interface DrilldownBreakdown {
  primaryDimension: BreakdownDimension;
  primaryValue: string;
  secondaryDimension: BreakdownDimension;
  items: UsageBreakdownItem[];
  total: { tokens: number; requests: number; cost: number };
}

export interface ComparativeBreakdown {
  dimension: BreakdownDimension;
  currentPeriod: { start: string; end: string };
  previousPeriod: { start: string; end: string };
  comparisons: ComparisonItem[];
  summary: {
    totalGrowth: number;
    growthPercent: number;
    newItems: number;
    churnedItems: number;
  };
}

export interface ComparisonItem {
  dimensionValue: string;
  current: UsageBreakdownItem | null;
  previous: UsageBreakdownItem | null;
  tokenChange: number;
  tokenChangePercent: number;
  requestChange: number;
  isNew?: boolean;
  isChurned?: boolean;
}

export const usageBreakdownService = new UsageBreakdownService();
```

---

## 7. Usage Trends Service

```typescript
// ============================================
// USAGE TRENDS SERVICE
// ============================================

export class UsageTrendsService {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // GET MULTI-METRIC TREND
  // ----------------------------------------
  async getMultiMetricTrend(
    orgId: string,
    startDate: string,
    endDate: string,
    metrics: ('tokens' | 'requests' | 'errors' | 'latency')[],
    granularity?: 'minute' | 'hour' | 'day' | 'week' | 'month'
  ): Promise<MultiMetricTrend> {
    const effectiveGranularity = granularity || this.inferGranularity(startDate, endDate);
    
    const trends: Record<string, UsageTrend> = {};
    
    for (const metric of metrics) {
      trends[metric] = await usageDashboardService.getUsageTrend(orgId, {
        startDate,
        endDate,
        granularity: effectiveGranularity,
        metric: metric as any
      });
    }
    
    // Calculate correlations
    const correlations = this.calculateCorrelations(trends);
    
    return {
      period: { startDate, endDate },
      granularity: effectiveGranularity,
      trends,
      correlations
    };
  }
  
  private inferGranularity(startDate: string, endDate: string): 'minute' | 'hour' | 'day' | 'week' | 'month' {
    const start = new Date(startDate);
    const end = new Date(endDate);
    const hours = (end.getTime() - start.getTime()) / (1000 * 60 * 60);
    
    if (hours <= 6) return 'minute';
    if (hours <= 168) return 'hour'; // 7 days
    if (hours <= 2160) return 'day'; // 90 days
    if (hours <= 8760) return 'week'; // 365 days
    return 'month';
  }
  
  private calculateCorrelations(trends: Record<string, UsageTrend>): CorrelationMatrix {
    const metrics = Object.keys(trends);
    const matrix: CorrelationMatrix = {};
    
    for (let i = 0; i < metrics.length; i++) {
      for (let j = i + 1; j < metrics.length; j++) {
        const metric1 = metrics[i];
        const metric2 = metrics[j];
        
        const values1 = trends[metric1].data.map(d => d.value);
        const values2 = trends[metric2].data.map(d => d.value);
        
        const correlation = this.pearsonCorrelation(values1, values2);
        
        const key = `${metric1}-${metric2}`;
        matrix[key] = {
          metrics: [metric1, metric2],
          correlation,
          strength: Math.abs(correlation) > 0.7 ? 'strong' : Math.abs(correlation) > 0.4 ? 'moderate' : 'weak'
        };
      }
    }
    
    return matrix;
  }
  
  private pearsonCorrelation(x: number[], y: number[]): number {
    if (x.length !== y.length || x.length === 0) return 0;
    
    const n = x.length;
    const sumX = x.reduce((a, b) => a + b, 0);
    const sumY = y.reduce((a, b) => a + b, 0);
    const sumXY = x.reduce((acc, xi, i) => acc + xi * y[i], 0);
    const sumX2 = x.reduce((acc, xi) => acc + xi * xi, 0);
    const sumY2 = y.reduce((acc, yi) => acc + yi * yi, 0);
    
    const numerator = n * sumXY - sumX * sumY;
    const denominator = Math.sqrt((n * sumX2 - sumX * sumX) * (n * sumY2 - sumY * sumY));
    
    return denominator === 0 ? 0 : numerator / denominator;
  }
  
  // ----------------------------------------
  // GET SEASONAL ANALYSIS
  // ----------------------------------------
  async getSeasonalAnalysis(
    orgId: string,
    months: number = 3
  ): Promise<SeasonalAnalysis> {
    const endDate = new Date();
    const startDate = new Date();
    startDate.setMonth(startDate.getMonth() - months);
    
    const { data, error } = await this.supabase
      .from('hourly_usage_rollup')
      .select('*')
      .eq('org_id', orgId)
      .gte('hour', startDate.toISOString())
      .lt('hour', endDate.toISOString());
    
    if (error) throw error;
    
    const rows = data || [];
    
    // Hourly patterns
    const hourlySeasonality = this.calculateHourlySeasonality(rows);
    
    // Daily patterns
    const dailySeasonality = this.calculateDailySeasonality(rows);
    
    // Weekly patterns
    const weeklySeasonality = this.calculateWeeklySeasonality(rows);
    
    // Identify peak periods
    const peakPeriods = this.identifyPeakPeriods(hourlySeasonality, dailySeasonality);
    
    return {
      period: { startDate: startDate.toISOString(), endDate: endDate.toISOString() },
      hourlySeasonality,
      dailySeasonality,
      weeklySeasonality,
      peakPeriods,
      recommendations: this.generateSeasonalRecommendations(peakPeriods)
    };
  }
  
  private calculateHourlySeasonality(rows: any[]): SeasonalPattern[] {
    const byHour = new Map<number, number[]>();
    
    for (const row of rows) {
      const hour = new Date(row.hour).getUTCHours();
      const existing = byHour.get(hour) || [];
      existing.push(Number(row.total_tokens));
      byHour.set(hour, existing);
    }
    
    const results: SeasonalPattern[] = [];
    const allAverages: number[] = [];
    
    for (let hour = 0; hour < 24; hour++) {
      const values = byHour.get(hour) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      allAverages.push(avg);
    }
    
    const overallAvg = allAverages.reduce((a, b) => a + b, 0) / 24;
    
    for (let hour = 0; hour < 24; hour++) {
      const values = byHour.get(hour) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      const variance = values.length > 0 
        ? values.reduce((sum, v) => sum + Math.pow(v - avg, 2), 0) / values.length 
        : 0;
      
      results.push({
        period: hour,
        label: `${hour.toString().padStart(2, '0')}:00`,
        avgValue: avg,
        variance,
        indexVsAverage: overallAvg > 0 ? (avg / overallAvg) * 100 : 0
      });
    }
    
    return results;
  }
  
  private calculateDailySeasonality(rows: any[]): SeasonalPattern[] {
    const byDay = new Map<number, number[]>();
    const dayNames = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'];
    
    for (const row of rows) {
      const day = new Date(row.hour).getUTCDay();
      const existing = byDay.get(day) || [];
      existing.push(Number(row.total_tokens));
      byDay.set(day, existing);
    }
    
    const results: SeasonalPattern[] = [];
    const allAverages: number[] = [];
    
    for (let day = 0; day < 7; day++) {
      const values = byDay.get(day) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      allAverages.push(avg);
    }
    
    const overallAvg = allAverages.reduce((a, b) => a + b, 0) / 7;
    
    for (let day = 0; day < 7; day++) {
      const values = byDay.get(day) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      
      results.push({
        period: day,
        label: dayNames[day],
        avgValue: avg,
        variance: 0,
        indexVsAverage: overallAvg > 0 ? (avg / overallAvg) * 100 : 0
      });
    }
    
    return results;
  }
  
  private calculateWeeklySeasonality(rows: any[]): SeasonalPattern[] {
    const byWeek = new Map<number, number[]>();
    
    for (const row of rows) {
      const date = new Date(row.hour);
      const week = this.getWeekNumber(date);
      const existing = byWeek.get(week) || [];
      existing.push(Number(row.total_tokens));
      byWeek.set(week, existing);
    }
    
    const weeks = Array.from(byWeek.keys()).sort((a, b) => a - b);
    const allAverages: number[] = [];
    
    for (const week of weeks) {
      const values = byWeek.get(week) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      allAverages.push(avg);
    }
    
    const overallAvg = allAverages.length > 0 
      ? allAverages.reduce((a, b) => a + b, 0) / allAverages.length 
      : 0;
    
    return weeks.map((week, index) => {
      const values = byWeek.get(week) || [];
      const avg = values.length > 0 ? values.reduce((a, b) => a + b, 0) / values.length : 0;
      
      return {
        period: week,
        label: `Week ${week}`,
        avgValue: avg,
        variance: 0,
        indexVsAverage: overallAvg > 0 ? (avg / overallAvg) * 100 : 0
      };
    });
  }
  
  private getWeekNumber(date: Date): number {
    const d = new Date(Date.UTC(date.getFullYear(), date.getMonth(), date.getDate()));
    const dayNum = d.getUTCDay() || 7;
    d.setUTCDate(d.getUTCDate() + 4 - dayNum);
    const yearStart = new Date(Date.UTC(d.getUTCFullYear(), 0, 1));
    return Math.ceil((((d.getTime() - yearStart.getTime()) / 86400000) + 1) / 7);
  }
  
  private identifyPeakPeriods(
    hourly: SeasonalPattern[], 
    daily: SeasonalPattern[]
  ): PeakPeriod[] {
    const peaks: PeakPeriod[] = [];
    
    // Find peak hours (>120% of average)
    const peakHours = hourly.filter(h => h.indexVsAverage > 120);
    if (peakHours.length > 0) {
      const peakStart = Math.min(...peakHours.map(h => h.period));
      const peakEnd = Math.max(...peakHours.map(h => h.period));
      
      peaks.push({
        type: 'hourly',
        label: `${peakStart.toString().padStart(2, '0')}:00 - ${peakEnd.toString().padStart(2, '0')}:59 UTC`,
        intensity: Math.max(...peakHours.map(h => h.indexVsAverage))
      });
    }
    
    // Find peak days
    const peakDays = daily.filter(d => d.indexVsAverage > 110);
    for (const day of peakDays) {
      peaks.push({
        type: 'daily',
        label: day.label,
        intensity: day.indexVsAverage
      });
    }
    
    return peaks.sort((a, b) => b.intensity - a.intensity);
  }
  
  private generateSeasonalRecommendations(peaks: PeakPeriod[]): string[] {
    const recommendations: string[] = [];
    
    const hourlyPeak = peaks.find(p => p.type === 'hourly');
    if (hourlyPeak && hourlyPeak.intensity > 150) {
      recommendations.push(
        `Consider pre-warming caches before ${hourlyPeak.label} when usage is ${hourlyPeak.intensity.toFixed(0)}% of average`
      );
    }
    
    const lowDays = peaks.filter(p => p.type === 'daily' && p.intensity < 80);
    if (lowDays.length > 0) {
      recommendations.push(
        `Schedule batch processing or maintenance during low-usage periods: ${lowDays.map(d => d.label).join(', ')}`
      );
    }
    
    return recommendations;
  }
}

// ============================================
// TREND TYPES
// ============================================

export interface MultiMetricTrend {
  period: { startDate: string; endDate: string };
  granularity: string;
  trends: Record<string, UsageTrend>;
  correlations: CorrelationMatrix;
}

export interface CorrelationMatrix {
  [key: string]: {
    metrics: string[];
    correlation: number;
    strength: 'strong' | 'moderate' | 'weak';
  };
}

export interface SeasonalAnalysis {
  period: { startDate: string; endDate: string };
  hourlySeasonality: SeasonalPattern[];
  dailySeasonality: SeasonalPattern[];
  weeklySeasonality: SeasonalPattern[];
  peakPeriods: PeakPeriod[];
  recommendations: string[];
}

export interface SeasonalPattern {
  period: number;
  label: string;
  avgValue: number;
  variance: number;
  indexVsAverage: number; // 100 = average, >100 = above average
}

export interface PeakPeriod {
  type: 'hourly' | 'daily' | 'weekly';
  label: string;
  intensity: number; // % of average
}

export const usageTrendsService = new UsageTrendsService();
```

---

## 8. Usage Quotas & Limits

```typescript
// ============================================
// USAGE QUOTAS SERVICE
// ============================================

export class UsageQuotasService {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // CREATE QUOTA
  // ----------------------------------------
  async createQuota(
    orgId: string,
    input: QuotaCreateInput,
    createdBy: string
  ): Promise<UsageQuota> {
    // Calculate initial period
    const { periodStart, periodEnd } = this.calculateCurrentPeriod(
      input.periodType,
      input.periodResetDay,
      input.periodResetHour
    );
    
    const { data, error } = await this.supabase
      .from('usage_quotas')
      .insert({
        org_id: orgId,
        name: input.name,
        description: input.description,
        scope_type: input.scopeType,
        team_id: input.teamId,
        project_id: input.projectId,
        user_identifier: input.userIdentifier,
        api_key_id: input.apiKeyId,
        feature: input.feature,
        token_limit: input.tokenLimit,
        request_limit: input.requestLimit,
        input_token_limit: input.inputTokenLimit,
        output_token_limit: input.outputTokenLimit,
        period_type: input.periodType,
        period_reset_day: input.periodResetDay || 1,
        period_reset_hour: input.periodResetHour || 0,
        enforcement_mode: input.enforcementMode || 'alert',
        warning_threshold_pct: input.warningThresholdPct || 80,
        critical_threshold_pct: input.criticalThresholdPct || 95,
        current_period_start: periodStart,
        current_period_end: periodEnd,
        created_by: createdBy
      })
      .select()
      .single();
    
    if (error) throw error;
    
    return this.mapToQuota(data);
  }
  
  private calculateCurrentPeriod(
    periodType: QuotaPeriodType,
    resetDay: number = 1,
    resetHour: number = 0
  ): { periodStart: string; periodEnd: string } {
    const now = new Date();
    let periodStart: Date;
    let periodEnd: Date;
    
    switch (periodType) {
      case 'hourly':
        periodStart = new Date(now);
        periodStart.setMinutes(0, 0, 0);
        periodEnd = new Date(periodStart);
        periodEnd.setHours(periodEnd.getHours() + 1);
        break;
        
      case 'daily':
        periodStart = new Date(now);
        periodStart.setUTCHours(resetHour, 0, 0, 0);
        if (now < periodStart) {
          periodStart.setDate(periodStart.getDate() - 1);
        }
        periodEnd = new Date(periodStart);
        periodEnd.setDate(periodEnd.getDate() + 1);
        break;
        
      case 'weekly':
        periodStart = new Date(now);
        const currentDay = periodStart.getUTCDay();
        const daysToSubtract = (currentDay - resetDay + 7) % 7;
        periodStart.setDate(periodStart.getDate() - daysToSubtract);
        periodStart.setUTCHours(resetHour, 0, 0, 0);
        if (now < periodStart) {
          periodStart.setDate(periodStart.getDate() - 7);
        }
        periodEnd = new Date(periodStart);
        periodEnd.setDate(periodEnd.getDate() + 7);
        break;
        
      case 'monthly':
        periodStart = new Date(now.getUTCFullYear(), now.getUTCMonth(), resetDay, resetHour);
        if (now < periodStart) {
          periodStart.setMonth(periodStart.getMonth() - 1);
        }
        periodEnd = new Date(periodStart);
        periodEnd.setMonth(periodEnd.getMonth() + 1);
        break;
    }
    
    return {
      periodStart: periodStart.toISOString(),
      periodEnd: periodEnd.toISOString()
    };
  }
  
  // ----------------------------------------
  // GET QUOTAS
  // ----------------------------------------
  async getQuotas(
    orgId: string,
    filters?: {
      scopeType?: QuotaScopeType;
      teamId?: string;
      projectId?: string;
      isActive?: boolean;
    }
  ): Promise<UsageQuota[]> {
    let query = this.supabase
      .from('usage_quotas')
      .select('*')
      .eq('org_id', orgId);
    
    if (filters?.scopeType) {
      query = query.eq('scope_type', filters.scopeType);
    }
    if (filters?.teamId) {
      query = query.eq('team_id', filters.teamId);
    }
    if (filters?.projectId) {
      query = query.eq('project_id', filters.projectId);
    }
    if (filters?.isActive !== undefined) {
      query = query.eq('is_active', filters.isActive);
    }
    
    const { data, error } = await query.order('created_at', { ascending: false });
    
    if (error) throw error;
    
    return (data || []).map(this.mapToQuota);
  }
  
  // ----------------------------------------
  // UPDATE QUOTA USAGE
  // ----------------------------------------
  async updateQuotaUsage(quotaId: string): Promise<QuotaStatus> {
    // Get quota configuration
    const { data: quota, error: quotaError } = await this.supabase
      .from('usage_quotas')
      .select('*')
      .eq('id', quotaId)
      .single();
    
    if (quotaError) throw quotaError;
    
    // Check if period needs reset
    const now = new Date();
    if (new Date(quota.current_period_end) <= now) {
      // Archive current period
      await this.archiveQuotaPeriod(quota);
      
      // Reset for new period
      const { periodStart, periodEnd } = this.calculateCurrentPeriod(
        quota.period_type,
        quota.period_reset_day,
        quota.period_reset_hour
      );
      
      await this.supabase
        .from('usage_quotas')
        .update({
          current_tokens: 0,
          current_requests: 0,
          current_input_tokens: 0,
          current_output_tokens: 0,
          current_period_start: periodStart,
          current_period_end: periodEnd,
          updated_at: new Date().toISOString()
        })
        .eq('id', quotaId);
      
      quota.current_period_start = periodStart;
      quota.current_period_end = periodEnd;
      quota.current_tokens = 0;
      quota.current_requests = 0;
    }
    
    // Calculate current usage for this period
    const usage = await this.calculateUsageForQuota(quota);
    
    // Update quota with current usage
    const { error: updateError } = await this.supabase
      .from('usage_quotas')
      .update({
        current_tokens: usage.totalTokens,
        current_requests: usage.requestCount,
        current_input_tokens: usage.inputTokens,
        current_output_tokens: usage.outputTokens,
        updated_at: new Date().toISOString()
      })
      .eq('id', quotaId);
    
    if (updateError) throw updateError;
    
    // Return status
    return this.getQuotaStatus(quotaId);
  }
  
  private async archiveQuotaPeriod(quota: any): Promise<void> {
    const tokenUtilization = quota.token_limit 
      ? (quota.current_tokens / quota.token_limit) * 100 
      : 0;
    const requestUtilization = quota.request_limit 
      ? (quota.current_requests / quota.request_limit) * 100 
      : 0;
    
    await this.supabase
      .from('quota_usage_history')
      .insert({
        quota_id: quota.id,
        org_id: quota.org_id,
        period_start: quota.current_period_start,
        period_end: quota.current_period_end,
        total_tokens: quota.current_tokens,
        total_requests: quota.current_requests,
        input_tokens: quota.current_input_tokens,
        output_tokens: quota.current_output_tokens,
        token_limit: quota.token_limit,
        request_limit: quota.request_limit,
        token_utilization_pct: tokenUtilization,
        request_utilization_pct: requestUtilization,
        exceeded_limit: tokenUtilization >= 100 || requestUtilization >= 100,
        max_utilization_pct: Math.max(tokenUtilization, requestUtilization)
      });
  }
  
  private async calculateUsageForQuota(quota: any): Promise<{
    totalTokens: number;
    inputTokens: number;
    outputTokens: number;
    requestCount: number;
  }> {
    let query = this.supabase
      .from('usage_records')
      .select('input_tokens, output_tokens, total_tokens, request_count')
      .eq('org_id', quota.org_id)
      .gte('timestamp', quota.current_period_start)
      .lt('timestamp', quota.current_period_end);
    
    // Apply scope filters
    switch (quota.scope_type) {
      case 'team':
        query = query.eq('team_id', quota.team_id);
        break;
      case 'project':
        query = query.eq('project_id', quota.project_id);
        break;
      case 'feature':
        query = query.eq('feature', quota.feature);
        break;
      case 'api_key':
        query = query.eq('api_key_id', quota.api_key_id);
        break;
      // 'organization' scope uses all records for the org
    }
    
    const { data, error } = await query;
    
    if (error) throw error;
    
    return {
      totalTokens: (data || []).reduce((sum, r) => sum + Number(r.total_tokens), 0),
      inputTokens: (data || []).reduce((sum, r) => sum + Number(r.input_tokens), 0),
      outputTokens: (data || []).reduce((sum, r) => sum + Number(r.output_tokens), 0),
      requestCount: (data || []).reduce((sum, r) => sum + Number(r.request_count), 0)
    };
  }
  
  // ----------------------------------------
  // GET QUOTA STATUS
  // ----------------------------------------
  async getQuotaStatus(quotaId: string): Promise<QuotaStatus> {
    const { data, error } = await this.supabase.rpc('check_quota_status', {
      p_quota_id: quotaId
    });
    
    if (error) throw error;
    
    const row = data[0];
    return {
      quotaId: row.quota_id,
      quotaName: row.quota_name,
      scopeType: row.scope_type,
      currentTokens: Number(row.current_tokens),
      tokenLimit: row.token_limit,
      tokenUtilizationPct: Number(row.token_utilization_pct),
      currentRequests: Number(row.current_requests),
      requestLimit: row.request_limit,
      requestUtilizationPct: Number(row.request_utilization_pct),
      status: row.status,
      periodRemainingHours: Number(row.period_remaining_hours)
    };
  }
  
  // ----------------------------------------
  // CHECK QUOTA BEFORE REQUEST
  // ----------------------------------------
  async checkQuotaBeforeRequest(
    orgId: string,
    context: {
      teamId?: string;
      projectId?: string;
      feature?: string;
      apiKeyId?: string;
      userIdentifier?: string;
      estimatedTokens: number;
    }
  ): Promise<QuotaCheckResult> {
    // Get all applicable quotas
    const { data: quotas, error } = await this.supabase
      .from('usage_quotas')
      .select('*')
      .eq('org_id', orgId)
      .eq('is_active', true);
    
    if (error) throw error;
    
    const applicableQuotas = (quotas || []).filter(q => 
      this.isQuotaApplicable(q, context)
    );
    
    const violations: QuotaViolation[] = [];
    const warnings: QuotaWarning[] = [];
    
    for (const quota of applicableQuotas) {
      const projectedTokens = quota.current_tokens + context.estimatedTokens;
      const projectedRequests = quota.current_requests + 1;
      
      // Check token limit
      if (quota.token_limit) {
        const utilizationPct = (projectedTokens / quota.token_limit) * 100;
        
        if (utilizationPct >= 100) {
          violations.push({
            quotaId: quota.id,
            quotaName: quota.name,
            limitType: 'tokens',
            currentUsage: quota.current_tokens,
            limit: quota.token_limit,
            projectedUsage: projectedTokens
          });
        } else if (utilizationPct >= quota.warning_threshold_pct) {
          warnings.push({
            quotaId: quota.id,
            quotaName: quota.name,
            limitType: 'tokens',
            utilizationPct,
            threshold: quota.warning_threshold_pct
          });
        }
      }
      
      // Check request limit
      if (quota.request_limit) {
        const utilizationPct = (projectedRequests / quota.request_limit) * 100;
        
        if (utilizationPct >= 100) {
          violations.push({
            quotaId: quota.id,
            quotaName: quota.name,
            limitType: 'requests',
            currentUsage: quota.current_requests,
            limit: quota.request_limit,
            projectedUsage: projectedRequests
          });
        }
      }
    }
    
    // Determine if request should be blocked
    const blockingViolations = violations.filter(v => {
      const quota = applicableQuotas.find(q => q.id === v.quotaId);
      return quota?.enforcement_mode === 'block';
    });
    
    return {
      allowed: blockingViolations.length === 0,
      violations,
      warnings,
      applicableQuotas: applicableQuotas.map(q => q.id)
    };
  }
  
  private isQuotaApplicable(quota: any, context: any): boolean {
    switch (quota.scope_type) {
      case 'organization':
        return true;
      case 'team':
        return quota.team_id === context.teamId;
      case 'project':
        return quota.project_id === context.projectId;
      case 'feature':
        return quota.feature === context.feature;
      case 'api_key':
        return quota.api_key_id === context.apiKeyId;
      case 'user':
        return quota.user_identifier === context.userIdentifier;
      default:
        return false;
    }
  }
  
  // ----------------------------------------
  // DELETE QUOTA
  // ----------------------------------------
  async deleteQuota(quotaId: string): Promise<void> {
    const { error } = await this.supabase
      .from('usage_quotas')
      .delete()
      .eq('id', quotaId);
    
    if (error) throw error;
  }
  
  // ----------------------------------------
  // GET QUOTA HISTORY
  // ----------------------------------------
  async getQuotaHistory(
    quotaId: string,
    limit: number = 12
  ): Promise<QuotaHistoryEntry[]> {
    const { data, error } = await this.supabase
      .from('quota_usage_history')
      .select('*')
      .eq('quota_id', quotaId)
      .order('period_start', { ascending: false })
      .limit(limit);
    
    if (error) throw error;
    
    return (data || []).map(row => ({
      periodStart: row.period_start,
      periodEnd: row.period_end,
      totalTokens: Number(row.total_tokens),
      totalRequests: Number(row.total_requests),
      tokenLimit: row.token_limit,
      requestLimit: row.request_limit,
      tokenUtilizationPct: Number(row.token_utilization_pct),
      requestUtilizationPct: Number(row.request_utilization_pct),
      exceededLimit: row.exceeded_limit,
      maxUtilizationPct: Number(row.max_utilization_pct)
    }));
  }
  
  private mapToQuota(row: any): UsageQuota {
    return {
      id: row.id,
      orgId: row.org_id,
      name: row.name,
      description: row.description,
      scopeType: row.scope_type,
      teamId: row.team_id,
      projectId: row.project_id,
      userIdentifier: row.user_identifier,
      apiKeyId: row.api_key_id,
      feature: row.feature,
      tokenLimit: row.token_limit,
      requestLimit: row.request_limit,
      inputTokenLimit: row.input_token_limit,
      outputTokenLimit: row.output_token_limit,
      periodType: row.period_type,
      periodResetDay: row.period_reset_day,
      periodResetHour: row.period_reset_hour,
      currentTokens: Number(row.current_tokens),
      currentRequests: Number(row.current_requests),
      currentInputTokens: Number(row.current_input_tokens),
      currentOutputTokens: Number(row.current_output_tokens),
      currentPeriodStart: row.current_period_start,
      currentPeriodEnd: row.current_period_end,
      enforcementMode: row.enforcement_mode,
      warningThresholdPct: row.warning_threshold_pct,
      criticalThresholdPct: row.critical_threshold_pct,
      isActive: row.is_active,
      createdBy: row.created_by,
      createdAt: row.created_at,
      updatedAt: row.updated_at
    };
  }
}

// ============================================
// QUOTA TYPES
// ============================================

export interface QuotaCheckResult {
  allowed: boolean;
  violations: QuotaViolation[];
  warnings: QuotaWarning[];
  applicableQuotas: string[];
}

export interface QuotaViolation {
  quotaId: string;
  quotaName: string;
  limitType: 'tokens' | 'requests';
  currentUsage: number;
  limit: number;
  projectedUsage: number;
}

export interface QuotaWarning {
  quotaId: string;
  quotaName: string;
  limitType: 'tokens' | 'requests';
  utilizationPct: number;
  threshold: number;
}

export interface QuotaHistoryEntry {
  periodStart: string;
  periodEnd: string;
  totalTokens: number;
  totalRequests: number;
  tokenLimit: number | null;
  requestLimit: number | null;
  tokenUtilizationPct: number;
  requestUtilizationPct: number;
  exceededLimit: boolean;
  maxUtilizationPct: number;
}

export const usageQuotasService = new UsageQuotasService();
```

---

## 9. Usage Attribution Service

```typescript
// ============================================
// USAGE ATTRIBUTION SERVICE
// ============================================

export class UsageAttributionService {
  private supabase: SupabaseClient;
  
  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // ATTRIBUTE USAGE FROM SDK
  // ----------------------------------------
  async attributeUsage(
    orgId: string,
    usageRecordId: string,
    attribution: UsageAttribution
  ): Promise<void> {
    const updates: Record<string, any> = {};
    
    if (attribution.teamId) updates.team_id = attribution.teamId;
    if (attribution.projectId) updates.project_id = attribution.projectId;
    if (attribution.costCenterId) updates.cost_center_id = attribution.costCenterId;
    if (attribution.feature) updates.feature = attribution.feature;
    if (attribution.environment) updates.environment = attribution.environment;
    if (attribution.userId) {
      updates.user_ids = [attribution.userId];
      updates.unique_user_count = 1;
    }
    if (attribution.metadata) {
      updates.provider_metadata = attribution.metadata;
    }
    
    updates.updated_at = new Date().toISOString();
    
    const { error } = await this.supabase
      .from('usage_records')
      .update(updates)
      .eq('id', usageRecordId)
      .eq('org_id', orgId);
    
    if (error) throw error;
    
    // Update user activity if userId provided
    if (attribution.userId) {
      await this.updateUserActivity(orgId, attribution.userId, usageRecordId);
    }
  }
  
  // ----------------------------------------
  // UPDATE USER ACTIVITY
  // ----------------------------------------
  private async updateUserActivity(
    orgId: string,
    userId: string,
    usageRecordId: string
  ): Promise<void> {
    // Get the usage record details
    const { data: record, error: recordError } = await this.supabase
      .from('usage_records')
      .select('*')
      .eq('id', usageRecordId)
      .single();
    
    if (recordError || !record) return;
    
    const activityDate = new Date(record.timestamp).toISOString().split('T')[0];
    
    // Upsert user activity
    const { data: existing, error: existingError } = await this.supabase
      .from('user_activity')
      .select('*')
      .eq('org_id', orgId)
      .eq('user_identifier', userId)
      .eq('activity_date', activityDate)
      .single();
    
    if (existingError && existingError.code !== 'PGRST116') throw existingError;
    
    if (existing) {
      // Update existing
      const modelUsage = existing.model_usage || {};
      const model = record.model || 'unknown';
      modelUsage[model] = {
        tokens: (modelUsage[model]?.tokens || 0) + Number(record.total_tokens),
        requests: (modelUsage[model]?.requests || 0) + Number(record.request_count)
      };
      
      const featureUsage = existing.feature_usage || {};
      const feature = record.feature || 'untagged';
      featureUsage[feature] = {
        tokens: (featureUsage[feature]?.tokens || 0) + Number(record.total_tokens),
        requests: (featureUsage[feature]?.requests || 0) + Number(record.request_count)
      };
      
      await this.supabase
        .from('user_activity')
        .update({
          total_tokens: existing.total_tokens + Number(record.total_tokens),
          input_tokens: existing.input_tokens + Number(record.input_tokens),
          output_tokens: existing.output_tokens + Number(record.output_tokens),
          cached_tokens: existing.cached_tokens + Number(record.cached_input_tokens),
          request_count: existing.request_count + Number(record.request_count),
          error_count: existing.error_count + Number(record.error_count),
          estimated_cost: existing.estimated_cost + Number(record.estimated_cost),
          model_usage: modelUsage,
          feature_usage: featureUsage,
          last_activity_at: record.timestamp,
          updated_at: new Date().toISOString()
        })
        .eq('id', existing.id);
    } else {
      // Create new
      const modelUsage: Record<string, any> = {};
      const model = record.model || 'unknown';
      modelUsage[model] = {
        tokens: Number(record.total_tokens),
        requests: Number(record.request_count)
      };
      
      const featureUsage: Record<string, any> = {};
      const feature = record.feature || 'untagged';
      featureUsage[feature] = {
        tokens: Number(record.total_tokens),
        requests: Number(record.request_count)
      };
      
      await this.supabase
        .from('user_activity')
        .insert({
          org_id: orgId,
          user_identifier: userId,
          activity_date: activityDate,
          total_tokens: Number(record.total_tokens),
          input_tokens: Number(record.input_tokens),
          output_tokens: Number(record.output_tokens),
          cached_tokens: Number(record.cached_input_tokens),
          request_count: Number(record.request_count),
          error_count: Number(record.error_count),
          estimated_cost: Number(record.estimated_cost),
          model_usage: modelUsage,
          feature_usage: featureUsage,
          first_activity_at: record.timestamp,
          last_activity_at: record.timestamp
        });
    }
  }
  
  // ----------------------------------------
  // GET USER ACTIVITY REPORT
  // ----------------------------------------
  async getUserActivityReport(
    orgId: string,
    startDate: string,
    endDate: string,
    options?: {
      userId?: string;
      teamId?: string;
      orderBy?: 'tokens' | 'requests' | 'cost';
      limit?: number;
    }
  ): Promise<UserActivityReport> {
    let query = this.supabase
      .from('user_activity')
      .select('*')
      .eq('org_id', orgId)
      .gte('activity_date', startDate)
      .lt('activity_date', endDate);
    
    if (options?.userId) {
      query = query.eq('user_identifier', options.userId);
    }
    
    const { data, error } = await query.order('activity_date', { ascending: false });
    
    if (error) throw error;
    
    // Aggregate by user
    const byUser = new Map<string, any>();
    
    for (const row of data || []) {
      const userId = row.user_identifier;
      const existing = byUser.get(userId) || {
        userIdentifier: userId,
        userDisplayName: row.user_display_name,
        totalTokens: 0,
        inputTokens: 0,
        outputTokens: 0,
        requestCount: 0,
        errorCount: 0,
        estimatedCost: 0,
        activeDays: 0,
        modelUsage: {},
        featureUsage: {},
        firstActivity: null,
        lastActivity: null
      };
      
      existing.totalTokens += Number(row.total_tokens);
      existing.inputTokens += Number(row.input_tokens);
      existing.outputTokens += Number(row.output_tokens);
      existing.requestCount += Number(row.request_count);
      existing.errorCount += Number(row.error_count);
      existing.estimatedCost += Number(row.estimated_cost);
      existing.activeDays += 1;
      
      // Merge model usage
      for (const [model, usage] of Object.entries(row.model_usage || {})) {
        const u = usage as any;
        existing.modelUsage[model] = {
          tokens: (existing.modelUsage[model]?.tokens || 0) + (u.tokens || 0),
          requests: (existing.modelUsage[model]?.requests || 0) + (u.requests || 0)
        };
      }
      
      // Merge feature usage
      for (const [feature, usage] of Object.entries(row.feature_usage || {})) {
        const u = usage as any;
        existing.featureUsage[feature] = {
          tokens: (existing.featureUsage[feature]?.tokens || 0) + (u.tokens || 0),
          requests: (existing.featureUsage[feature]?.requests || 0) + (u.requests || 0)
        };
      }
      
      // Track first/last activity
      if (!existing.firstActivity || row.first_activity_at < existing.firstActivity) {
        existing.firstActivity = row.first_activity_at;
      }
      if (!existing.lastActivity || row.last_activity_at > existing.lastActivity) {
        existing.lastActivity = row.last_activity_at;
      }
      
      byUser.set(userId, existing);
    }
    
    // Sort and limit
    let users = Array.from(byUser.values());
    
    const orderBy = options?.orderBy || 'tokens';
    users.sort((a, b) => {
      switch (orderBy) {
        case 'tokens': return b.totalTokens - a.totalTokens;
        case 'requests': return b.requestCount - a.requestCount;
        case 'cost': return b.estimatedCost - a.estimatedCost;
        default: return b.totalTokens - a.totalTokens;
      }
    });
    
    if (options?.limit) {
      users = users.slice(0, options.limit);
    }
    
    // Calculate totals
    const totals = {
      totalUsers: users.length,
      totalTokens: users.reduce((sum, u) => sum + u.totalTokens, 0),
      totalRequests: users.reduce((sum, u) => sum + u.requestCount, 0),
      totalCost: users.reduce((sum, u) => sum + u.estimatedCost, 0),
      avgTokensPerUser: 0,
      avgCostPerUser: 0
    };
    
    if (totals.totalUsers > 0) {
      totals.avgTokensPerUser = totals.totalTokens / totals.totalUsers;
      totals.avgCostPerUser = totals.totalCost / totals.totalUsers;
    }
    
    return {
      period: { startDate, endDate },
      users,
      totals
    };
  }
  
  // ----------------------------------------
  // GET ATTRIBUTION COVERAGE
  // ----------------------------------------
  async getAttributionCoverage(
    orgId: string,
    startDate: string,
    endDate: string
  ): Promise<AttributionCoverage> {
    const { data, error } = await this.supabase
      .from('usage_records')
      .select(`
        id,
        team_id,
        project_id,
        cost_center_id,
        feature,
        user_ids,
        total_tokens,
        request_count
      `)
      .eq('org_id', orgId)
      .gte('timestamp', startDate)
      .lt('timestamp', endDate);
    
    if (error) throw error;
    
    const records = data || [];
    const totalRecords = records.length;
    const totalTokens = records.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    
    // Calculate coverage for each dimension
    const teamCoverage = this.calculateDimensionCoverage(records, 'team_id', totalTokens);
    const projectCoverage = this.calculateDimensionCoverage(records, 'project_id', totalTokens);
    const costCenterCoverage = this.calculateDimensionCoverage(records, 'cost_center_id', totalTokens);
    const featureCoverage = this.calculateDimensionCoverage(records, 'feature', totalTokens);
    const userCoverage = this.calculateUserCoverage(records, totalTokens);
    
    // Overall coverage score (weighted average)
    const overallScore = (
      teamCoverage.tokenCoveragePct * 0.3 +
      projectCoverage.tokenCoveragePct * 0.2 +
      featureCoverage.tokenCoveragePct * 0.3 +
      userCoverage.tokenCoveragePct * 0.2
    );
    
    return {
      period: { startDate, endDate },
      totalRecords,
      totalTokens,
      dimensions: {
        team: teamCoverage,
        project: projectCoverage,
        costCenter: costCenterCoverage,
        feature: featureCoverage,
        user: userCoverage
      },
      overallScore,
      recommendations: this.generateCoverageRecommendations({
        team: teamCoverage,
        project: projectCoverage,
        feature: featureCoverage,
        user: userCoverage
      })
    };
  }
  
  private calculateDimensionCoverage(
    records: any[],
    dimension: string,
    totalTokens: number
  ): DimensionCoverage {
    const attributed = records.filter(r => r[dimension] !== null);
    const attributedTokens = attributed.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    
    return {
      attributedRecords: attributed.length,
      totalRecords: records.length,
      recordCoveragePct: records.length > 0 ? (attributed.length / records.length) * 100 : 0,
      attributedTokens,
      totalTokens,
      tokenCoveragePct: totalTokens > 0 ? (attributedTokens / totalTokens) * 100 : 0
    };
  }
  
  private calculateUserCoverage(records: any[], totalTokens: number): DimensionCoverage {
    const attributed = records.filter(r => r.user_ids && r.user_ids.length > 0);
    const attributedTokens = attributed.reduce((sum, r) => sum + Number(r.total_tokens), 0);
    
    return {
      attributedRecords: attributed.length,
      totalRecords: records.length,
      recordCoveragePct: records.length > 0 ? (attributed.length / records.length) * 100 : 0,
      attributedTokens,
      totalTokens,
      tokenCoveragePct: totalTokens > 0 ? (attributedTokens / totalTokens) * 100 : 0
    };
  }
  
  private generateCoverageRecommendations(
    coverage: Record<string, DimensionCoverage>
  ): string[] {
    const recommendations: string[] = [];
    
    if (coverage.team.tokenCoveragePct < 50) {
      recommendations.push(
        `Only ${coverage.team.tokenCoveragePct.toFixed(0)}% of tokens are attributed to teams. Add team tags to your SDK calls.`
      );
    }
    
    if (coverage.feature.tokenCoveragePct < 70) {
      recommendations.push(
        `${(100 - coverage.feature.tokenCoveragePct).toFixed(0)}% of tokens have no feature attribution. Tag requests with feature names for better analysis.`
      );
    }
    
    if (coverage.user.tokenCoveragePct < 30) {
      recommendations.push(
        `User attribution is low (${coverage.user.tokenCoveragePct.toFixed(0)}%). Add userId to SDK calls for per-user analytics.`
      );
    }
    
    return recommendations;
  }
}

// ============================================
// ATTRIBUTION TYPES
// ============================================

export interface UsageAttribution {
  teamId?: string;
  projectId?: string;
  costCenterId?: string;
  feature?: string;
  environment?: string;
  userId?: string;
  metadata?: Record<string, unknown>;
}

export interface UserActivityReport {
  period: { startDate: string; endDate: string };
  users: UserActivitySummary[];
  totals: {
    totalUsers: number;
    totalTokens: number;
    totalRequests: number;
    totalCost: number;
    avgTokensPerUser: number;
    avgCostPerUser: number;
  };
}

export interface UserActivitySummary {
  userIdentifier: string;
  userDisplayName?: string;
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  requestCount: number;
  errorCount: number;
  estimatedCost: number;
  activeDays: number;
  modelUsage: Record<string, { tokens: number; requests: number }>;
  featureUsage: Record<string, { tokens: number; requests: number }>;
  firstActivity: string | null;
  lastActivity: string | null;
}

export interface AttributionCoverage {
  period: { startDate: string; endDate: string };
  totalRecords: number;
  totalTokens: number;
  dimensions: {
    team: DimensionCoverage;
    project: DimensionCoverage;
    costCenter: DimensionCoverage;
    feature: DimensionCoverage;
    user: DimensionCoverage;
  };
  overallScore: number;
  recommendations: string[];
}

export interface DimensionCoverage {
  attributedRecords: number;
  totalRecords: number;
  recordCoveragePct: number;
  attributedTokens: number;
  totalTokens: number;
  tokenCoveragePct: number;
}

export const usageAttributionService = new UsageAttributionService();
```

---

## 10. Real-Time Usage Monitoring

```typescript
// ============================================
// REAL-TIME USAGE SERVICE
// ============================================

import Redis from 'ioredis';

export class RealTimeUsageService {
  private redis: Redis;
  private supabase: SupabaseClient;
  
  constructor() {
    this.redis = new Redis(process.env.REDIS_URL!);
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
  }
  
  // ----------------------------------------
  // RECORD REAL-TIME USAGE
  // ----------------------------------------
  async recordUsage(
    orgId: string,
    usage: RealTimeUsageEvent
  ): Promise<void> {
    const now = Date.now();
    const minute = Math.floor(now / 60000) * 60000;
    
    // Update minute-level counters in Redis
    const key = `usage:${orgId}:${minute}`;
    const multi = this.redis.multi();
    
    multi.hincrby(key, 'tokens', usage.totalTokens);
    multi.hincrby(key, 'input_tokens', usage.inputTokens);
    multi.hincrby(key, 'output_tokens', usage.outputTokens);
    multi.hincrby(key, 'requests', 1);
    if (usage.error) {
      multi.hincrby(key, 'errors', 1);
    }
    multi.hincrby(key, 'latency_sum', usage.latencyMs || 0);
    multi.expire(key, 86400); // 24 hour TTL
    
    await multi.exec();
    
    // Update model-specific counters
    if (usage.model) {
      const modelKey = `usage:${orgId}:model:${usage.model}:${minute}`;
      await this.redis.hincrby(modelKey, 'tokens', usage.totalTokens);
      await this.redis.hincrby(modelKey, 'requests', 1);
      await this.redis.expire(modelKey, 86400);
    }
    
    // Update feature-specific counters
    if (usage.feature) {
      const featureKey = `usage:${orgId}:feature:${usage.feature}:${minute}`;
      await this.redis.hincrby(featureKey, 'tokens', usage.totalTokens);
      await this.redis.hincrby(featureKey, 'requests', 1);
      await this.redis.expire(featureKey, 86400);
    }
    
    // Publish to real-time channel
    await this.redis.publish(`usage:${orgId}`, JSON.stringify({
      timestamp: now,
      ...usage
    }));
  }
  
  // ----------------------------------------
  // GET REAL-TIME METRICS
  // ----------------------------------------
  async getRealTimeMetrics(
    orgId: string,
    windowMinutes: number = 5
  ): Promise<RealTimeMetrics> {
    const now = Date.now();
    const windowStart = now - (windowMinutes * 60 * 1000);
    
    // Collect metrics from Redis for each minute in window
    const metrics: RealTimeMetrics = {
      windowStart: new Date(windowStart).toISOString(),
      windowEnd: new Date(now).toISOString(),
      totalTokens: 0,
      inputTokens: 0,
      outputTokens: 0,
      requestCount: 0,
      errorCount: 0,
      avgLatencyMs: 0,
      tokensPerMinute: 0,
      requestsPerMinute: 0,
      minuteData: []
    };
    
    let totalLatency = 0;
    
    for (let minute = Math.floor(windowStart / 60000) * 60000; minute <= now; minute += 60000) {
      const key = `usage:${orgId}:${minute}`;
      const data = await this.redis.hgetall(key);
      
      if (Object.keys(data).length > 0) {
        const tokens = parseInt(data.tokens || '0');
        const inputTokens = parseInt(data.input_tokens || '0');
        const outputTokens = parseInt(data.output_tokens || '0');
        const requests = parseInt(data.requests || '0');
        const errors = parseInt(data.errors || '0');
        const latencySum = parseInt(data.latency_sum || '0');
        
        metrics.totalTokens += tokens;
        metrics.inputTokens += inputTokens;
        metrics.outputTokens += outputTokens;
        metrics.requestCount += requests;
        metrics.errorCount += errors;
        totalLatency += latencySum;
        
        metrics.minuteData.push({
          timestamp: new Date(minute).toISOString(),
          tokens,
          requests,
          errors,
          avgLatency: requests > 0 ? latencySum / requests : 0
        });
      }
    }
    
    // Calculate derived metrics
    if (metrics.requestCount > 0) {
      metrics.avgLatencyMs = totalLatency / metrics.requestCount;
    }
    
    metrics.tokensPerMinute = metrics.totalTokens / windowMinutes;
    metrics.requestsPerMinute = metrics.requestCount / windowMinutes;
    
    return metrics;
  }
  
  // ----------------------------------------
  // GET LIVE STREAM
  // ----------------------------------------
  async subscribeLiveUsage(
    orgId: string,
    callback: (event: RealTimeUsageEvent) => void
  ): Promise<() => void> {
    const subscriber = this.redis.duplicate();
    const channel = `usage:${orgId}`;
    
    await subscriber.subscribe(channel);
    
    subscriber.on('message', (ch, message) => {
      if (ch === channel) {
        try {
          const event = JSON.parse(message);
          callback(event);
        } catch (e) {
          console.error('Failed to parse usage event:', e);
        }
      }
    });
    
    // Return unsubscribe function
    return async () => {
      await subscriber.unsubscribe(channel);
      await subscriber.quit();
    };
  }
  
  // ----------------------------------------
  // GET CURRENT RATE
  // ----------------------------------------
  async getCurrentRate(orgId: string): Promise<RateMetrics> {
    const now = Date.now();
    const currentMinute = Math.floor(now / 60000) * 60000;
    const lastMinute = currentMinute - 60000;
    
    // Get current and last minute data
    const [currentData, lastData] = await Promise.all([
      this.redis.hgetall(`usage:${orgId}:${currentMinute}`),
      this.redis.hgetall(`usage:${orgId}:${lastMinute}`)
    ]);
    
    const currentTokens = parseInt(currentData.tokens || '0');
    const currentRequests = parseInt(currentData.requests || '0');
    const lastTokens = parseInt(lastData.tokens || '0');
    const lastRequests = parseInt(lastData.requests || '0');
    
    // Calculate rates (extrapolate current minute)
    const secondsIntoMinute = (now % 60000) / 1000;
    const projectedTokens = secondsIntoMinute > 0 
      ? (currentTokens / secondsIntoMinute) * 60 
      : currentTokens;
    const projectedRequests = secondsIntoMinute > 0 
      ? (currentRequests / secondsIntoMinute) * 60 
      : currentRequests;
    
    return {
      currentMinuteTokens: currentTokens,
      currentMinuteRequests: currentRequests,
      lastMinuteTokens: lastTokens,
      lastMinuteRequests: lastRequests,
      projectedMinuteTokens: Math.round(projectedTokens),
      projectedMinuteRequests: Math.round(projectedRequests),
      tokenRateChange: lastTokens > 0 
        ? ((projectedTokens - lastTokens) / lastTokens) * 100 
        : 0,
      requestRateChange: lastRequests > 0 
        ? ((projectedRequests - lastRequests) / lastRequests) * 100 
        : 0
    };
  }
  
  // ----------------------------------------
  // GET MODEL BREAKDOWN (REAL-TIME)
  // ----------------------------------------
  async getRealTimeModelBreakdown(
    orgId: string,
    windowMinutes: number = 5
  ): Promise<RealTimeModelBreakdown[]> {
    const now = Date.now();
    const windowStart = now - (windowMinutes * 60 * 1000);
    
    // Get all model keys for this org
    const pattern = `usage:${orgId}:model:*`;
    const keys = await this.redis.keys(pattern);
    
    const modelData = new Map<string, { tokens: number; requests: number }>();
    
    for (const key of keys) {
      // Extract model name and timestamp from key
      const parts = key.split(':');
      const model = parts[3];
      const timestamp = parseInt(parts[4]);
      
      if (timestamp >= Math.floor(windowStart / 60000) * 60000) {
        const data = await this.redis.hgetall(key);
        const existing = modelData.get(model) || { tokens: 0, requests: 0 };
        modelData.set(model, {
          tokens: existing.tokens + parseInt(data.tokens || '0'),
          requests: existing.requests + parseInt(data.requests || '0')
        });
      }
    }
    
    // Sort by tokens descending
    const totalTokens = Array.from(modelData.values()).reduce((sum, m) => sum + m.tokens, 0);
    
    return Array.from(modelData.entries())
      .map(([model, data]) => ({
        model,
        tokens: data.tokens,
        requests: data.requests,
        percentage: totalTokens > 0 ? (data.tokens / totalTokens) * 100 : 0
      }))
      .sort((a, b) => b.tokens - a.tokens);
  }
  
  // ----------------------------------------
  // FLUSH TO DATABASE
  // ----------------------------------------
  async flushToDatabase(orgId: string): Promise<void> {
    const now = Date.now();
    const flushBefore = now - (5 * 60 * 1000); // Flush data older than 5 minutes
    
    // Get all minute keys for this org
    const pattern = `usage:${orgId}:[0-9]*`;
    const keys = await this.redis.keys(pattern);
    
    for (const key of keys) {
      const timestamp = parseInt(key.split(':')[2]);
      
      if (timestamp < flushBefore) {
        const data = await this.redis.hgetall(key);
        
        if (Object.keys(data).length > 0) {
          // Insert or update usage_records
          await this.supabase
            .from('usage_records')
            .upsert({
              org_id: orgId,
              timestamp: new Date(timestamp).toISOString(),
              bucket_start: new Date(timestamp).toISOString(),
              bucket_end: new Date(timestamp + 60000).toISOString(),
              granularity: '1m',
              provider: 'aggregated', // Real-time data may be aggregated
              model: 'aggregated',
              input_tokens: parseInt(data.input_tokens || '0'),
              output_tokens: parseInt(data.output_tokens || '0'),
              request_count: parseInt(data.requests || '0'),
              error_count: parseInt(data.errors || '0'),
              total_latency_ms: parseInt(data.latency_sum || '0')
            }, {
              onConflict: 'org_id,timestamp,provider,model'
            });
          
          // Delete from Redis
          await this.redis.del(key);
        }
      }
    }
  }
}

// ============================================
// REAL-TIME TYPES
// ============================================

export interface RealTimeUsageEvent {
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  model?: string;
  provider?: string;
  feature?: string;
  userId?: string;
  latencyMs?: number;
  error?: boolean;
  errorMessage?: string;
}

export interface RealTimeMetrics {
  windowStart: string;
  windowEnd: string;
  totalTokens: number;
  inputTokens: number;
  outputTokens: number;
  requestCount: number;
  errorCount: number;
  avgLatencyMs: number;
  tokensPerMinute: number;
  requestsPerMinute: number;
  minuteData: MinuteData[];
}

export interface MinuteData {
  timestamp: string;
  tokens: number;
  requests: number;
  errors: number;
  avgLatency: number;
}

export interface RateMetrics {
  currentMinuteTokens: number;
  currentMinuteRequests: number;
  lastMinuteTokens: number;
  lastMinuteRequests: number;
  projectedMinuteTokens: number;
  projectedMinuteRequests: number;
  tokenRateChange: number;
  requestRateChange: number;
}

export interface RealTimeModelBreakdown {
  model: string;
  tokens: number;
  requests: number;
  percentage: number;
}

export const realTimeUsageService = new RealTimeUsageService();
```

---

## 11. Usage Comparison Service

The Usage Comparison Service enables period-over-period analysis to identify growth trends, efficiency improvements, and anomalous usage patterns.

### 11.1 Comparison Service Implementation

```typescript
// ============================================
// FILE: services/usage/usageComparisonService.ts
// Usage period-over-period comparison engine
// ============================================

import { createClient } from '@supabase/supabase-js';
import { Database } from '@/types/database';

const supabase = createClient<Database>(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

// ============================================
// TYPES
// ============================================

export type ComparisonType = 
  | 'period_over_period'  // Compare sequential periods
  | 'year_over_year'      // Same period, different years
  | 'custom'              // Custom date ranges
  | 'benchmark';          // Compare against baseline

export type ChangeDirection = 'increase' | 'decrease' | 'stable';

export interface DateRange {
  start: Date;
  end: Date;
}

export interface ComparisonPeriods {
  current: DateRange;
  previous: DateRange;
}

export interface UsageComparisonRequest {
  orgId: string;
  comparisonType: ComparisonType;
  currentPeriod?: DateRange;
  previousPeriod?: DateRange;
  dimensions?: string[];
  metrics?: string[];
  filters?: Record<string, string[]>;
}

export interface MetricComparison {
  metric: string;
  currentValue: number;
  previousValue: number;
  absoluteChange: number;
  percentageChange: number;
  direction: ChangeDirection;
  isSignificant: boolean;      // Change > threshold
  significanceLevel: 'low' | 'medium' | 'high';
}

export interface UsageComparisonResult {
  periods: ComparisonPeriods;
  summary: {
    totalMetricsCompared: number;
    improvements: number;
    regressions: number;
    stable: number;
    overallHealth: 'improving' | 'stable' | 'declining';
  };
  metrics: MetricComparison[];
  dimensionComparisons?: DimensionComparison[];
  insights: ComparisonInsight[];
  recommendations: ComparisonRecommendation[];
}

export interface DimensionComparison {
  dimension: string;
  items: DimensionItemComparison[];
  newItems: string[];
  removedItems: string[];
  topGrowers: { name: string; growth: number }[];
  topDecliners: { name: string; decline: number }[];
}

export interface DimensionItemComparison {
  name: string;
  currentTokens: number;
  previousTokens: number;
  currentRequests: number;
  previousRequests: number;
  tokenChange: number;
  requestChange: number;
  percentageOfTotal: {
    current: number;
    previous: number;
  };
}

export interface ComparisonInsight {
  type: 'growth' | 'efficiency' | 'anomaly' | 'shift' | 'optimization';
  severity: 'info' | 'warning' | 'critical';
  title: string;
  description: string;
  metric?: string;
  dimension?: string;
  impact?: {
    tokens?: number;
    requests?: number;
    percentage?: number;
  };
}

export interface ComparisonRecommendation {
  priority: 'low' | 'medium' | 'high' | 'critical';
  category: string;
  title: string;
  description: string;
  expectedImpact: string;
  actionItems: string[];
}

// ============================================
// USAGE COMPARISON SERVICE
// ============================================

export class UsageComparisonService {
  
  // Significance thresholds
  private readonly CHANGE_THRESHOLDS = {
    low: 0.05,      // 5%
    medium: 0.15,   // 15%
    high: 0.30      // 30%
  };
  
  // ==========================================
  // PERIOD CALCULATION
  // ==========================================
  
  calculateComparisonPeriods(
    comparisonType: ComparisonType,
    currentPeriod?: DateRange,
    previousPeriod?: DateRange
  ): ComparisonPeriods {
    const now = new Date();
    
    switch (comparisonType) {
      case 'period_over_period': {
        // Last 30 days vs previous 30 days
        const periodLength = 30 * 24 * 60 * 60 * 1000; // 30 days in ms
        return {
          current: {
            start: new Date(now.getTime() - periodLength),
            end: now
          },
          previous: {
            start: new Date(now.getTime() - 2 * periodLength),
            end: new Date(now.getTime() - periodLength)
          }
        };
      }
      
      case 'year_over_year': {
        // This month vs same month last year
        const currentMonthStart = new Date(now.getFullYear(), now.getMonth(), 1);
        const previousYearMonthStart = new Date(now.getFullYear() - 1, now.getMonth(), 1);
        const previousYearMonthEnd = new Date(now.getFullYear() - 1, now.getMonth() + 1, 0);
        
        return {
          current: {
            start: currentMonthStart,
            end: now
          },
          previous: {
            start: previousYearMonthStart,
            end: previousYearMonthEnd
          }
        };
      }
      
      case 'custom': {
        if (!currentPeriod || !previousPeriod) {
          throw new Error('Custom comparison requires both periods');
        }
        return {
          current: currentPeriod,
          previous: previousPeriod
        };
      }
      
      case 'benchmark': {
        // Compare against a baseline period (e.g., first month of usage)
        // This would typically be configurable per org
        const currentMonthStart = new Date(now.getFullYear(), now.getMonth(), 1);
        return {
          current: {
            start: currentMonthStart,
            end: now
          },
          previous: previousPeriod || {
            start: new Date(now.getFullYear(), 0, 1), // Jan 1
            end: new Date(now.getFullYear(), 1, 0)    // Jan 31
          }
        };
      }
      
      default:
        throw new Error(`Unknown comparison type: ${comparisonType}`);
    }
  }
  
  // ==========================================
  // MAIN COMPARISON
  // ==========================================
  
  async compareUsage(request: UsageComparisonRequest): Promise<UsageComparisonResult> {
    const periods = this.calculateComparisonPeriods(
      request.comparisonType,
      request.currentPeriod,
      request.previousPeriod
    );
    
    // Fetch metrics for both periods
    const [currentMetrics, previousMetrics] = await Promise.all([
      this.fetchPeriodMetrics(request.orgId, periods.current, request.filters),
      this.fetchPeriodMetrics(request.orgId, periods.previous, request.filters)
    ]);
    
    // Compare core metrics
    const metricComparisons = this.compareMetrics(currentMetrics, previousMetrics);
    
    // Compare dimensions if requested
    let dimensionComparisons: DimensionComparison[] | undefined;
    if (request.dimensions && request.dimensions.length > 0) {
      dimensionComparisons = await this.compareDimensions(
        request.orgId,
        periods,
        request.dimensions,
        request.filters
      );
    }
    
    // Generate insights
    const insights = this.generateInsights(
      metricComparisons,
      dimensionComparisons,
      periods
    );
    
    // Generate recommendations
    const recommendations = this.generateRecommendations(
      metricComparisons,
      dimensionComparisons,
      insights
    );
    
    // Calculate summary
    const summary = this.calculateSummary(metricComparisons);
    
    return {
      periods,
      summary,
      metrics: metricComparisons,
      dimensionComparisons,
      insights,
      recommendations
    };
  }
  
  // ==========================================
  // DATA FETCHING
  // ==========================================
  
  private async fetchPeriodMetrics(
    orgId: string,
    period: DateRange,
    filters?: Record<string, string[]>
  ): Promise<Record<string, number>> {
    let query = supabase
      .from('daily_usage_rollup')
      .select('*')
      .eq('org_id', orgId)
      .gte('date', period.start.toISOString().split('T')[0])
      .lte('date', period.end.toISOString().split('T')[0]);
    
    // Apply filters
    if (filters) {
      Object.entries(filters).forEach(([key, values]) => {
        if (values.length > 0) {
          query = query.in(key, values);
        }
      });
    }
    
    const { data, error } = await query;
    
    if (error) throw error;
    
    // Aggregate metrics
    return this.aggregateMetrics(data || []);
  }
  
  private aggregateMetrics(records: any[]): Record<string, number> {
    const totals: Record<string, number> = {
      totalTokens: 0,
      inputTokens: 0,
      outputTokens: 0,
      cachedTokens: 0,
      totalRequests: 0,
      successRequests: 0,
      errorRequests: 0,
      totalLatencyMs: 0,
      uniqueModels: new Set(),
      uniqueUsers: new Set(),
      activeDays: 0
    } as any;
    
    const latencies: number[] = [];
    const models = new Set<string>();
    const users = new Set<string>();
    
    records.forEach(r => {
      totals.totalTokens += r.total_tokens || 0;
      totals.inputTokens += r.input_tokens || 0;
      totals.outputTokens += r.output_tokens || 0;
      totals.cachedTokens += r.cached_tokens || 0;
      totals.totalRequests += r.request_count || 0;
      totals.successRequests += r.success_count || 0;
      totals.errorRequests += r.error_count || 0;
      totals.totalLatencyMs += r.total_latency_ms || 0;
      
      if (r.model) models.add(r.model);
      if (r.user_id) users.add(r.user_id);
      if (r.avg_latency_ms) latencies.push(r.avg_latency_ms);
    });
    
    // Calculate derived metrics
    const daysInPeriod = records.length > 0 ? 
      new Set(records.map(r => r.date)).size : 1;
    
    return {
      totalTokens: totals.totalTokens,
      inputTokens: totals.inputTokens,
      outputTokens: totals.outputTokens,
      cachedTokens: totals.cachedTokens,
      totalRequests: totals.totalRequests,
      successRequests: totals.successRequests,
      errorRequests: totals.errorRequests,
      errorRate: totals.totalRequests > 0 ? 
        (totals.errorRequests / totals.totalRequests) * 100 : 0,
      avgLatencyMs: latencies.length > 0 ?
        latencies.reduce((a, b) => a + b, 0) / latencies.length : 0,
      tokensPerRequest: totals.totalRequests > 0 ?
        totals.totalTokens / totals.totalRequests : 0,
      tokensPerDay: totals.totalTokens / daysInPeriod,
      requestsPerDay: totals.totalRequests / daysInPeriod,
      cacheHitRate: totals.totalTokens > 0 ?
        (totals.cachedTokens / totals.totalTokens) * 100 : 0,
      uniqueModels: models.size,
      uniqueUsers: users.size,
      activeDays: daysInPeriod
    };
  }
  
  // ==========================================
  // METRIC COMPARISON
  // ==========================================
  
  private compareMetrics(
    current: Record<string, number>,
    previous: Record<string, number>
  ): MetricComparison[] {
    const metricsToCompare = [
      'totalTokens', 'inputTokens', 'outputTokens', 'cachedTokens',
      'totalRequests', 'errorRate', 'avgLatencyMs', 'tokensPerRequest',
      'tokensPerDay', 'requestsPerDay', 'cacheHitRate', 'uniqueModels',
      'uniqueUsers', 'activeDays'
    ];
    
    return metricsToCompare.map(metric => {
      const currentValue = current[metric] || 0;
      const previousValue = previous[metric] || 0;
      const absoluteChange = currentValue - previousValue;
      const percentageChange = previousValue !== 0 ?
        ((currentValue - previousValue) / previousValue) * 100 : 
        (currentValue > 0 ? 100 : 0);
      
      // Determine direction
      let direction: ChangeDirection;
      if (Math.abs(percentageChange) < 1) {
        direction = 'stable';
      } else if (percentageChange > 0) {
        direction = 'increase';
      } else {
        direction = 'decrease';
      }
      
      // Determine significance
      const absPercentage = Math.abs(percentageChange) / 100;
      let significanceLevel: 'low' | 'medium' | 'high';
      if (absPercentage >= this.CHANGE_THRESHOLDS.high) {
        significanceLevel = 'high';
      } else if (absPercentage >= this.CHANGE_THRESHOLDS.medium) {
        significanceLevel = 'medium';
      } else {
        significanceLevel = 'low';
      }
      
      return {
        metric,
        currentValue,
        previousValue,
        absoluteChange,
        percentageChange,
        direction,
        isSignificant: absPercentage >= this.CHANGE_THRESHOLDS.low,
        significanceLevel
      };
    });
  }
  
  // ==========================================
  // DIMENSION COMPARISON
  // ==========================================
  
  private async compareDimensions(
    orgId: string,
    periods: ComparisonPeriods,
    dimensions: string[],
    filters?: Record<string, string[]>
  ): Promise<DimensionComparison[]> {
    const results: DimensionComparison[] = [];
    
    for (const dimension of dimensions) {
      const [currentData, previousData] = await Promise.all([
        this.fetchDimensionBreakdown(orgId, periods.current, dimension, filters),
        this.fetchDimensionBreakdown(orgId, periods.previous, dimension, filters)
      ]);
      
      // Get all unique items
      const allItems = new Set([
        ...Object.keys(currentData),
        ...Object.keys(previousData)
      ]);
      
      // Calculate totals
      const currentTotal = Object.values(currentData)
        .reduce((sum, d: any) => sum + d.tokens, 0);
      const previousTotal = Object.values(previousData)
        .reduce((sum, d: any) => sum + d.tokens, 0);
      
      // Compare each item
      const items: DimensionItemComparison[] = [];
      const newItems: string[] = [];
      const removedItems: string[] = [];
      
      allItems.forEach(item => {
        const current = currentData[item] || { tokens: 0, requests: 0 };
        const previous = previousData[item] || { tokens: 0, requests: 0 };
        
        if (!previousData[item]) {
          newItems.push(item);
        } else if (!currentData[item]) {
          removedItems.push(item);
        }
        
        items.push({
          name: item,
          currentTokens: current.tokens,
          previousTokens: previous.tokens,
          currentRequests: current.requests,
          previousRequests: previous.requests,
          tokenChange: previous.tokens > 0 ?
            ((current.tokens - previous.tokens) / previous.tokens) * 100 : 100,
          requestChange: previous.requests > 0 ?
            ((current.requests - previous.requests) / previous.requests) * 100 : 100,
          percentageOfTotal: {
            current: currentTotal > 0 ? (current.tokens / currentTotal) * 100 : 0,
            previous: previousTotal > 0 ? (previous.tokens / previousTotal) * 100 : 0
          }
        });
      });
      
      // Sort for top growers/decliners
      const sortedByGrowth = [...items]
        .filter(i => i.previousTokens > 0)
        .sort((a, b) => b.tokenChange - a.tokenChange);
      
      results.push({
        dimension,
        items,
        newItems,
        removedItems,
        topGrowers: sortedByGrowth.slice(0, 5).map(i => ({
          name: i.name,
          growth: i.tokenChange
        })),
        topDecliners: sortedByGrowth.slice(-5).reverse().map(i => ({
          name: i.name,
          decline: i.tokenChange
        }))
      });
    }
    
    return results;
  }
  
  private async fetchDimensionBreakdown(
    orgId: string,
    period: DateRange,
    dimension: string,
    filters?: Record<string, string[]>
  ): Promise<Record<string, { tokens: number; requests: number }>> {
    let query = supabase
      .from('daily_usage_rollup')
      .select(`${dimension}, total_tokens, request_count`)
      .eq('org_id', orgId)
      .gte('date', period.start.toISOString().split('T')[0])
      .lte('date', period.end.toISOString().split('T')[0])
      .not(dimension, 'is', null);
    
    if (filters) {
      Object.entries(filters).forEach(([key, values]) => {
        if (values.length > 0) {
          query = query.in(key, values);
        }
      });
    }
    
    const { data, error } = await query;
    if (error) throw error;
    
    // Aggregate by dimension
    const breakdown: Record<string, { tokens: number; requests: number }> = {};
    
    (data || []).forEach(row => {
      const key = row[dimension] as string;
      if (!breakdown[key]) {
        breakdown[key] = { tokens: 0, requests: 0 };
      }
      breakdown[key].tokens += row.total_tokens || 0;
      breakdown[key].requests += row.request_count || 0;
    });
    
    return breakdown;
  }
  
  // ==========================================
  // INSIGHT GENERATION
  // ==========================================
  
  private generateInsights(
    metrics: MetricComparison[],
    dimensions?: DimensionComparison[],
    periods?: ComparisonPeriods
  ): ComparisonInsight[] {
    const insights: ComparisonInsight[] = [];
    
    // Analyze token growth
    const tokenMetric = metrics.find(m => m.metric === 'totalTokens');
    if (tokenMetric && tokenMetric.isSignificant) {
      if (tokenMetric.direction === 'increase') {
        insights.push({
          type: 'growth',
          severity: tokenMetric.significanceLevel === 'high' ? 'warning' : 'info',
          title: 'Significant Usage Growth',
          description: `Token consumption increased by ${tokenMetric.percentageChange.toFixed(1)}% compared to the previous period`,
          metric: 'totalTokens',
          impact: {
            tokens: tokenMetric.absoluteChange,
            percentage: tokenMetric.percentageChange
          }
        });
      }
    }
    
    // Analyze error rate
    const errorMetric = metrics.find(m => m.metric === 'errorRate');
    if (errorMetric && errorMetric.direction === 'increase' && errorMetric.currentValue > 1) {
      insights.push({
        type: 'anomaly',
        severity: errorMetric.currentValue > 5 ? 'critical' : 'warning',
        title: 'Increased Error Rate',
        description: `Error rate increased from ${errorMetric.previousValue.toFixed(2)}% to ${errorMetric.currentValue.toFixed(2)}%`,
        metric: 'errorRate',
        impact: {
          percentage: errorMetric.percentageChange
        }
      });
    }
    
    // Analyze cache efficiency
    const cacheMetric = metrics.find(m => m.metric === 'cacheHitRate');
    if (cacheMetric) {
      if (cacheMetric.direction === 'increase' && cacheMetric.isSignificant) {
        insights.push({
          type: 'efficiency',
          severity: 'info',
          title: 'Cache Efficiency Improved',
          description: `Cache hit rate improved from ${cacheMetric.previousValue.toFixed(1)}% to ${cacheMetric.currentValue.toFixed(1)}%`,
          metric: 'cacheHitRate'
        });
      } else if (cacheMetric.direction === 'decrease' && cacheMetric.isSignificant) {
        insights.push({
          type: 'efficiency',
          severity: 'warning',
          title: 'Cache Efficiency Declined',
          description: `Cache hit rate dropped from ${cacheMetric.previousValue.toFixed(1)}% to ${cacheMetric.currentValue.toFixed(1)}%`,
          metric: 'cacheHitRate'
        });
      }
    }
    
    // Analyze latency
    const latencyMetric = metrics.find(m => m.metric === 'avgLatencyMs');
    if (latencyMetric && latencyMetric.direction === 'increase' && 
        latencyMetric.percentageChange > 20) {
      insights.push({
        type: 'anomaly',
        severity: 'warning',
        title: 'Latency Increase Detected',
        description: `Average latency increased by ${latencyMetric.percentageChange.toFixed(1)}% (${latencyMetric.previousValue.toFixed(0)}ms  ${latencyMetric.currentValue.toFixed(0)}ms)`,
        metric: 'avgLatencyMs'
      });
    }
    
    // Analyze tokens per request
    const tprMetric = metrics.find(m => m.metric === 'tokensPerRequest');
    if (tprMetric && tprMetric.isSignificant) {
      if (tprMetric.direction === 'increase') {
        insights.push({
          type: 'shift',
          severity: 'info',
          title: 'Prompt Complexity Increased',
          description: `Average tokens per request increased from ${tprMetric.previousValue.toFixed(0)} to ${tprMetric.currentValue.toFixed(0)}`,
          metric: 'tokensPerRequest'
        });
      } else if (tprMetric.direction === 'decrease') {
        insights.push({
          type: 'optimization',
          severity: 'info',
          title: 'Prompt Efficiency Improved',
          description: `Average tokens per request decreased from ${tprMetric.previousValue.toFixed(0)} to ${tprMetric.currentValue.toFixed(0)}`,
          metric: 'tokensPerRequest'
        });
      }
    }
    
    // Analyze dimension shifts
    if (dimensions) {
      dimensions.forEach(dim => {
        // Check for new high-volume items
        dim.newItems.forEach(item => {
          const itemData = dim.items.find(i => i.name === item);
          if (itemData && itemData.percentageOfTotal.current > 10) {
            insights.push({
              type: 'shift',
              severity: 'info',
              title: `New ${dim.dimension} Emerged`,
              description: `"${item}" is new and accounts for ${itemData.percentageOfTotal.current.toFixed(1)}% of total usage`,
              dimension: dim.dimension
            });
          }
        });
        
        // Check for concentration changes
        const topItems = dim.items
          .sort((a, b) => b.currentTokens - a.currentTokens)
          .slice(0, 3);
        const topConcentration = topItems.reduce(
          (sum, i) => sum + i.percentageOfTotal.current, 0
        );
        const prevTopConcentration = topItems.reduce(
          (sum, i) => sum + i.percentageOfTotal.previous, 0
        );
        
        if (topConcentration - prevTopConcentration > 15) {
          insights.push({
            type: 'shift',
            severity: 'warning',
            title: `${dim.dimension} Concentration Increased`,
            description: `Top 3 ${dim.dimension}s now account for ${topConcentration.toFixed(1)}% of usage (was ${prevTopConcentration.toFixed(1)}%)`,
            dimension: dim.dimension
          });
        }
      });
    }
    
    return insights;
  }
  
  // ==========================================
  // RECOMMENDATION GENERATION
  // ==========================================
  
  private generateRecommendations(
    metrics: MetricComparison[],
    dimensions?: DimensionComparison[],
    insights?: ComparisonInsight[]
  ): ComparisonRecommendation[] {
    const recommendations: ComparisonRecommendation[] = [];
    
    // Check error rate
    const errorMetric = metrics.find(m => m.metric === 'errorRate');
    if (errorMetric && errorMetric.currentValue > 2) {
      recommendations.push({
        priority: errorMetric.currentValue > 5 ? 'critical' : 'high',
        category: 'Reliability',
        title: 'Investigate Error Rate',
        description: `Current error rate of ${errorMetric.currentValue.toFixed(2)}% is impacting reliability`,
        expectedImpact: 'Improve success rate and reduce wasted requests',
        actionItems: [
          'Review error logs for common failure patterns',
          'Check provider status pages for incidents',
          'Implement retry logic with exponential backoff',
          'Set up alerting for error rate thresholds'
        ]
      });
    }
    
    // Check cache efficiency
    const cacheMetric = metrics.find(m => m.metric === 'cacheHitRate');
    if (cacheMetric && cacheMetric.currentValue < 30) {
      recommendations.push({
        priority: 'medium',
        category: 'Efficiency',
        title: 'Improve Cache Hit Rate',
        description: `Cache hit rate of ${cacheMetric.currentValue.toFixed(1)}% indicates optimization opportunity`,
        expectedImpact: 'Reduce token consumption by 20-40%',
        actionItems: [
          'Implement semantic caching for similar queries',
          'Normalize prompts before caching',
          'Increase cache TTL for stable responses',
          'Pre-warm cache with common queries'
        ]
      });
    }
    
    // Check token growth
    const tokenMetric = metrics.find(m => m.metric === 'totalTokens');
    const requestMetric = metrics.find(m => m.metric === 'totalRequests');
    if (tokenMetric && requestMetric) {
      const tokenGrowth = tokenMetric.percentageChange;
      const requestGrowth = requestMetric.percentageChange;
      
      // Token growth outpacing request growth
      if (tokenGrowth > requestGrowth + 20) {
        recommendations.push({
          priority: 'medium',
          category: 'Optimization',
          title: 'Optimize Token Usage',
          description: `Token growth (${tokenGrowth.toFixed(1)}%) exceeds request growth (${requestGrowth.toFixed(1)}%), indicating less efficient prompts`,
          expectedImpact: 'Reduce cost per request by 15-25%',
          actionItems: [
            'Review and optimize system prompts',
            'Implement prompt compression techniques',
            'Consider using smaller models for simple tasks',
            'Audit longest prompts for reduction opportunities'
          ]
        });
      }
    }
    
    // Check for model diversification opportunity
    if (dimensions) {
      const modelDim = dimensions.find(d => d.dimension === 'model');
      if (modelDim) {
        const topModel = modelDim.items
          .sort((a, b) => b.currentTokens - a.currentTokens)[0];
        if (topModel && topModel.percentageOfTotal.current > 80) {
          recommendations.push({
            priority: 'medium',
            category: 'Model Strategy',
            title: 'Diversify Model Usage',
            description: `${topModel.name} accounts for ${topModel.percentageOfTotal.current.toFixed(1)}% of usage. Consider task-appropriate model routing`,
            expectedImpact: 'Reduce costs by 20-40% for suitable tasks',
            actionItems: [
              'Identify tasks suitable for smaller models',
              'Implement complexity-based model routing',
              'A/B test quality with cheaper alternatives',
              'Set up model usage quotas per task type'
            ]
          });
        }
      }
    }
    
    // Sort by priority
    const priorityOrder = { critical: 0, high: 1, medium: 2, low: 3 };
    recommendations.sort((a, b) => priorityOrder[a.priority] - priorityOrder[b.priority]);
    
    return recommendations;
  }
  
  // ==========================================
  // SUMMARY CALCULATION
  // ==========================================
  
  private calculateSummary(metrics: MetricComparison[]): UsageComparisonResult['summary'] {
    const significantMetrics = metrics.filter(m => m.isSignificant);
    
    // Define which metrics are "good" when increasing vs decreasing
    const goodWhenDecreasing = ['errorRate', 'avgLatencyMs', 'tokensPerRequest'];
    const goodWhenIncreasing = ['cacheHitRate', 'successRate'];
    
    let improvements = 0;
    let regressions = 0;
    let stable = 0;
    
    metrics.forEach(m => {
      if (!m.isSignificant) {
        stable++;
      } else if (goodWhenDecreasing.includes(m.metric)) {
        if (m.direction === 'decrease') improvements++;
        else if (m.direction === 'increase') regressions++;
      } else if (goodWhenIncreasing.includes(m.metric)) {
        if (m.direction === 'increase') improvements++;
        else if (m.direction === 'decrease') regressions++;
      } else {
        // Neutral metrics (e.g., total tokens) - track but don't count as improvement/regression
        stable++;
      }
    });
    
    let overallHealth: 'improving' | 'stable' | 'declining';
    if (improvements > regressions * 2) {
      overallHealth = 'improving';
    } else if (regressions > improvements * 2) {
      overallHealth = 'declining';
    } else {
      overallHealth = 'stable';
    }
    
    return {
      totalMetricsCompared: metrics.length,
      improvements,
      regressions,
      stable,
      overallHealth
    };
  }
}

export const usageComparisonService = new UsageComparisonService();
```

---

## 12. Usage Export Service

The Usage Export Service provides comprehensive data export capabilities for reporting, compliance, and integration with external systems.

### 12.1 Export Service Implementation

```typescript
// ============================================
// FILE: services/usage/usageExportService.ts
// Usage data export for reporting & compliance
// ============================================

import { createClient } from '@supabase/supabase-js';
import { Database } from '@/types/database';
import { createObjectCsvStringifier } from 'csv-writer';
import ExcelJS from 'exceljs';

const supabase = createClient<Database>(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

// ============================================
// TYPES
// ============================================

export type ExportFormat = 'csv' | 'xlsx' | 'json' | 'parquet';
export type ExportGranularity = 'minute' | 'hourly' | 'daily' | 'monthly';

export interface ExportRequest {
  orgId: string;
  format: ExportFormat;
  startDate: Date;
  endDate: Date;
  granularity: ExportGranularity;
  dimensions?: string[];
  metrics?: string[];
  filters?: Record<string, string[]>;
  includeMetadata?: boolean;
  fileName?: string;
}

export interface ExportResult {
  success: boolean;
  fileName: string;
  fileSize: number;
  rowCount: number;
  downloadUrl?: string;
  expiresAt?: Date;
  error?: string;
}

export interface ScheduledExport {
  id: string;
  orgId: string;
  name: string;
  schedule: string;  // Cron expression
  exportConfig: Omit<ExportRequest, 'orgId' | 'startDate' | 'endDate'>;
  relativePeriod: 'last_day' | 'last_week' | 'last_month' | 'last_quarter';
  deliveryMethod: 'download' | 'email' | 's3' | 'webhook';
  deliveryConfig: Record<string, string>;
  enabled: boolean;
  lastRunAt?: Date;
  nextRunAt?: Date;
}

// ============================================
// USAGE EXPORT SERVICE
// ============================================

export class UsageExportService {
  
  private readonly DEFAULT_METRICS = [
    'total_tokens', 'input_tokens', 'output_tokens', 'cached_tokens',
    'request_count', 'success_count', 'error_count',
    'avg_latency_ms', 'p95_latency_ms'
  ];
  
  private readonly DEFAULT_DIMENSIONS = [
    'provider', 'model', 'team', 'project', 'feature', 'environment'
  ];
  
  // ==========================================
  // MAIN EXPORT
  // ==========================================
  
  async exportUsage(request: ExportRequest): Promise<ExportResult> {
    try {
      // Fetch data
      const data = await this.fetchExportData(request);
      
      if (data.length === 0) {
        return {
          success: false,
          fileName: '',
          fileSize: 0,
          rowCount: 0,
          error: 'No data found for the specified criteria'
        };
      }
      
      // Generate file based on format
      let result: { buffer: Buffer; mimeType: string };
      
      switch (request.format) {
        case 'csv':
          result = await this.generateCsv(data, request);
          break;
        case 'xlsx':
          result = await this.generateXlsx(data, request);
          break;
        case 'json':
          result = await this.generateJson(data, request);
          break;
        case 'parquet':
          result = await this.generateParquet(data, request);
          break;
        default:
          throw new Error(`Unsupported format: ${request.format}`);
      }
      
      // Generate filename
      const fileName = this.generateFileName(request);
      
      // Upload to storage
      const { downloadUrl, expiresAt } = await this.uploadToStorage(
        fileName,
        result.buffer,
        result.mimeType,
        request.orgId
      );
      
      return {
        success: true,
        fileName,
        fileSize: result.buffer.length,
        rowCount: data.length,
        downloadUrl,
        expiresAt
      };
      
    } catch (error) {
      console.error('Export failed:', error);
      return {
        success: false,
        fileName: '',
        fileSize: 0,
        rowCount: 0,
        error: error instanceof Error ? error.message : 'Export failed'
      };
    }
  }
  
  // ==========================================
  // DATA FETCHING
  // ==========================================
  
  private async fetchExportData(request: ExportRequest): Promise<any[]> {
    // Determine which table to query based on granularity
    const table = this.getTableForGranularity(request.granularity);
    
    // Build select clause
    const metrics = request.metrics || this.DEFAULT_METRICS;
    const dimensions = request.dimensions || this.DEFAULT_DIMENSIONS;
    const selectFields = [
      ...dimensions.filter(d => d !== 'date' && d !== 'timestamp'),
      ...metrics
    ];
    
    // Add date/time field based on table
    if (table === 'usage_records') {
      selectFields.unshift('timestamp', 'bucket_start', 'bucket_end');
    } else {
      selectFields.unshift('date');
    }
    
    let query = supabase
      .from(table)
      .select(selectFields.join(','))
      .eq('org_id', request.orgId)
      .order(table === 'usage_records' ? 'timestamp' : 'date', { ascending: true });
    
    // Apply date filters
    if (table === 'usage_records') {
      query = query
        .gte('timestamp', request.startDate.toISOString())
        .lte('timestamp', request.endDate.toISOString());
    } else {
      query = query
        .gte('date', request.startDate.toISOString().split('T')[0])
        .lte('date', request.endDate.toISOString().split('T')[0]);
    }
    
    // Apply dimension filters
    if (request.filters) {
      Object.entries(request.filters).forEach(([key, values]) => {
        if (values.length > 0) {
          query = query.in(key, values);
        }
      });
    }
    
    // Limit for safety
    query = query.limit(1000000);
    
    const { data, error } = await query;
    
    if (error) throw error;
    
    // Transform data for export
    return this.transformDataForExport(data || [], request);
  }
  
  private getTableForGranularity(granularity: ExportGranularity): string {
    switch (granularity) {
      case 'minute':
        return 'usage_records';
      case 'hourly':
        return 'hourly_usage_rollup';
      case 'daily':
      case 'monthly':
        return 'daily_usage_rollup';
      default:
        return 'daily_usage_rollup';
    }
  }
  
  private transformDataForExport(data: any[], request: ExportRequest): any[] {
    return data.map(row => {
      const transformed: Record<string, any> = {};
      
      // Format date/time
      if (row.timestamp) {
        transformed['Timestamp'] = new Date(row.timestamp).toISOString();
      } else if (row.date) {
        transformed['Date'] = row.date;
      }
      
      // Map dimensions with friendly names
      const dimensionNames: Record<string, string> = {
        provider: 'Provider',
        model: 'Model',
        team: 'Team',
        project: 'Project',
        feature: 'Feature',
        environment: 'Environment',
        user_id: 'User ID',
        api_key_id: 'API Key ID',
        cost_center: 'Cost Center'
      };
      
      const dimensions = request.dimensions || this.DEFAULT_DIMENSIONS;
      dimensions.forEach(dim => {
        if (row[dim] !== undefined) {
          transformed[dimensionNames[dim] || dim] = row[dim] || 'N/A';
        }
      });
      
      // Map metrics with friendly names
      const metricNames: Record<string, string> = {
        total_tokens: 'Total Tokens',
        input_tokens: 'Input Tokens',
        output_tokens: 'Output Tokens',
        cached_tokens: 'Cached Tokens',
        cache_creation_tokens: 'Cache Creation Tokens',
        request_count: 'Requests',
        success_count: 'Successful Requests',
        error_count: 'Failed Requests',
        avg_latency_ms: 'Avg Latency (ms)',
        p50_latency_ms: 'P50 Latency (ms)',
        p95_latency_ms: 'P95 Latency (ms)',
        p99_latency_ms: 'P99 Latency (ms)',
        estimated_cost: 'Estimated Cost ($)'
      };
      
      const metrics = request.metrics || this.DEFAULT_METRICS;
      metrics.forEach(metric => {
        if (row[metric] !== undefined) {
          const value = row[metric];
          // Format numbers appropriately
          if (metric.includes('cost')) {
            transformed[metricNames[metric] || metric] = 
              typeof value === 'number' ? value.toFixed(4) : value;
          } else if (metric.includes('latency')) {
            transformed[metricNames[metric] || metric] = 
              typeof value === 'number' ? Math.round(value) : value;
          } else {
            transformed[metricNames[metric] || metric] = value;
          }
        }
      });
      
      return transformed;
    });
  }
  
  // ==========================================
  // FILE GENERATION
  // ==========================================
  
  private async generateCsv(
    data: any[],
    request: ExportRequest
  ): Promise<{ buffer: Buffer; mimeType: string }> {
    if (data.length === 0) {
      return { buffer: Buffer.from(''), mimeType: 'text/csv' };
    }
    
    const headers = Object.keys(data[0]).map(key => ({
      id: key,
      title: key
    }));
    
    const csvStringifier = createObjectCsvStringifier({ header: headers });
    const csvString = 
      csvStringifier.getHeaderString() + 
      csvStringifier.stringifyRecords(data);
    
    return {
      buffer: Buffer.from(csvString, 'utf-8'),
      mimeType: 'text/csv'
    };
  }
  
  private async generateXlsx(
    data: any[],
    request: ExportRequest
  ): Promise<{ buffer: Buffer; mimeType: string }> {
    const workbook = new ExcelJS.Workbook();
    workbook.creator = 'TokenTra';
    workbook.created = new Date();
    
    // Main data sheet
    const sheet = workbook.addWorksheet('Usage Data');
    
    if (data.length > 0) {
      // Add headers
      const columns = Object.keys(data[0]).map(key => ({
        header: key,
        key,
        width: Math.max(key.length, 15)
      }));
      sheet.columns = columns;
      
      // Style header row
      sheet.getRow(1).font = { bold: true };
      sheet.getRow(1).fill = {
        type: 'pattern',
        pattern: 'solid',
        fgColor: { argb: 'FFE0E0E0' }
      };
      
      // Add data rows
      data.forEach(row => {
        sheet.addRow(row);
      });
      
      // Auto-filter
      sheet.autoFilter = {
        from: { row: 1, column: 1 },
        to: { row: data.length + 1, column: columns.length }
      };
      
      // Format number columns
      sheet.columns.forEach((col, index) => {
        const header = columns[index]?.header || '';
        if (header.includes('Tokens') || header.includes('Requests')) {
          col.numFmt = '#,##0';
        } else if (header.includes('Cost')) {
          col.numFmt = '$#,##0.0000';
        } else if (header.includes('Latency')) {
          col.numFmt = '#,##0';
        }
      });
    }
    
    // Add metadata sheet if requested
    if (request.includeMetadata) {
      const metaSheet = workbook.addWorksheet('Export Metadata');
      metaSheet.columns = [
        { header: 'Property', key: 'property', width: 25 },
        { header: 'Value', key: 'value', width: 50 }
      ];
      metaSheet.getRow(1).font = { bold: true };
      
      metaSheet.addRows([
        { property: 'Export Date', value: new Date().toISOString() },
        { property: 'Organization', value: request.orgId },
        { property: 'Start Date', value: request.startDate.toISOString() },
        { property: 'End Date', value: request.endDate.toISOString() },
        { property: 'Granularity', value: request.granularity },
        { property: 'Total Rows', value: data.length },
        { property: 'Dimensions', value: (request.dimensions || this.DEFAULT_DIMENSIONS).join(', ') },
        { property: 'Metrics', value: (request.metrics || this.DEFAULT_METRICS).join(', ') }
      ]);
    }
    
    const buffer = await workbook.xlsx.writeBuffer();
    
    return {
      buffer: Buffer.from(buffer),
      mimeType: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    };
  }
  
  private async generateJson(
    data: any[],
    request: ExportRequest
  ): Promise<{ buffer: Buffer; mimeType: string }> {
    const output = {
      metadata: request.includeMetadata ? {
        exportDate: new Date().toISOString(),
        organization: request.orgId,
        startDate: request.startDate.toISOString(),
        endDate: request.endDate.toISOString(),
        granularity: request.granularity,
        totalRows: data.length
      } : undefined,
      data
    };
    
    return {
      buffer: Buffer.from(JSON.stringify(output, null, 2), 'utf-8'),
      mimeType: 'application/json'
    };
  }
  
  private async generateParquet(
    data: any[],
    request: ExportRequest
  ): Promise<{ buffer: Buffer; mimeType: string }> {
    // Note: In production, use parquetjs or apache-arrow
    // This is a simplified implementation
    const parquet = require('parquetjs-lite');
    
    if (data.length === 0) {
      throw new Error('Cannot generate Parquet file with no data');
    }
    
    // Infer schema from data
    const schemaFields: Record<string, any> = {};
    Object.keys(data[0]).forEach(key => {
      const sampleValue = data[0][key];
      if (typeof sampleValue === 'number') {
        schemaFields[key] = { type: 'DOUBLE' };
      } else if (sampleValue instanceof Date) {
        schemaFields[key] = { type: 'TIMESTAMP_MILLIS' };
      } else {
        schemaFields[key] = { type: 'UTF8' };
      }
    });
    
    const schema = new parquet.ParquetSchema(schemaFields);
    
    // Write to buffer
    const writer = await parquet.ParquetWriter.openBuffer(schema);
    
    for (const row of data) {
      await writer.appendRow(row);
    }
    
    await writer.close();
    
    return {
      buffer: writer.buffer,
      mimeType: 'application/vnd.apache.parquet'
    };
  }
  
  // ==========================================
  // STORAGE
  // ==========================================
  
  private generateFileName(request: ExportRequest): string {
    const dateStr = new Date().toISOString().split('T')[0];
    const startStr = request.startDate.toISOString().split('T')[0];
    const endStr = request.endDate.toISOString().split('T')[0];
    
    if (request.fileName) {
      return `${request.fileName}.${request.format}`;
    }
    
    return `tokentra_usage_${startStr}_to_${endStr}_${dateStr}.${request.format}`;
  }
  
  private async uploadToStorage(
    fileName: string,
    buffer: Buffer,
    mimeType: string,
    orgId: string
  ): Promise<{ downloadUrl: string; expiresAt: Date }> {
    const path = `exports/${orgId}/${fileName}`;
    
    const { error: uploadError } = await supabase.storage
      .from('exports')
      .upload(path, buffer, {
        contentType: mimeType,
        upsert: true
      });
    
    if (uploadError) throw uploadError;
    
    // Generate signed URL (expires in 24 hours)
    const expiresIn = 24 * 60 * 60; // 24 hours in seconds
    const { data: urlData, error: urlError } = await supabase.storage
      .from('exports')
      .createSignedUrl(path, expiresIn);
    
    if (urlError) throw urlError;
    
    const expiresAt = new Date(Date.now() + expiresIn * 1000);
    
    return {
      downloadUrl: urlData.signedUrl,
      expiresAt
    };
  }
  
  // ==========================================
  // SCHEDULED EXPORTS
  // ==========================================
  
  async createScheduledExport(
    config: Omit<ScheduledExport, 'id' | 'lastRunAt' | 'nextRunAt'>
  ): Promise<ScheduledExport> {
    const nextRunAt = this.calculateNextRun(config.schedule);
    
    const { data, error } = await supabase
      .from('scheduled_exports')
      .insert({
        ...config,
        next_run_at: nextRunAt.toISOString()
      })
      .select()
      .single();
    
    if (error) throw error;
    return this.mapScheduledExport(data);
  }
  
  async runScheduledExport(exportId: string): Promise<ExportResult> {
    // Fetch config
    const { data: config, error } = await supabase
      .from('scheduled_exports')
      .select('*')
      .eq('id', exportId)
      .single();
    
    if (error) throw error;
    if (!config) throw new Error('Scheduled export not found');
    
    // Calculate date range based on relative period
    const { startDate, endDate } = this.calculateRelativePeriod(
      config.relative_period
    );
    
    // Run export
    const result = await this.exportUsage({
      orgId: config.org_id,
      format: config.export_config.format,
      startDate,
      endDate,
      granularity: config.export_config.granularity,
      dimensions: config.export_config.dimensions,
      metrics: config.export_config.metrics,
      filters: config.export_config.filters,
      includeMetadata: config.export_config.includeMetadata
    });
    
    // Deliver based on method
    if (result.success && result.downloadUrl) {
      await this.deliverExport(
        result,
        config.delivery_method,
        config.delivery_config
      );
    }
    
    // Update last/next run
    const nextRunAt = this.calculateNextRun(config.schedule);
    await supabase
      .from('scheduled_exports')
      .update({
        last_run_at: new Date().toISOString(),
        next_run_at: nextRunAt.toISOString()
      })
      .eq('id', exportId);
    
    return result;
  }
  
  private calculateRelativePeriod(
    period: ScheduledExport['relativePeriod']
  ): { startDate: Date; endDate: Date } {
    const now = new Date();
    let startDate: Date;
    let endDate: Date;
    
    switch (period) {
      case 'last_day':
        endDate = new Date(now.getFullYear(), now.getMonth(), now.getDate());
        startDate = new Date(endDate.getTime() - 24 * 60 * 60 * 1000);
        break;
      case 'last_week':
        endDate = new Date(now.getFullYear(), now.getMonth(), now.getDate());
        startDate = new Date(endDate.getTime() - 7 * 24 * 60 * 60 * 1000);
        break;
      case 'last_month':
        endDate = new Date(now.getFullYear(), now.getMonth(), 1);
        startDate = new Date(now.getFullYear(), now.getMonth() - 1, 1);
        break;
      case 'last_quarter':
        const currentQuarter = Math.floor(now.getMonth() / 3);
        endDate = new Date(now.getFullYear(), currentQuarter * 3, 1);
        startDate = new Date(now.getFullYear(), (currentQuarter - 1) * 3, 1);
        break;
      default:
        endDate = now;
        startDate = new Date(now.getTime() - 24 * 60 * 60 * 1000);
    }
    
    return { startDate, endDate };
  }
  
  private calculateNextRun(cronExpression: string): Date {
    // Simplified cron parsing - in production use a library like cron-parser
    const parser = require('cron-parser');
    const interval = parser.parseExpression(cronExpression);
    return interval.next().toDate();
  }
  
  private async deliverExport(
    result: ExportResult,
    method: ScheduledExport['deliveryMethod'],
    config: Record<string, string>
  ): Promise<void> {
    switch (method) {
      case 'email':
        await this.sendEmailDelivery(result, config.email);
        break;
      case 's3':
        await this.uploadToS3(result, config);
        break;
      case 'webhook':
        await this.sendWebhook(result, config.webhookUrl);
        break;
      case 'download':
        // No action needed - URL is already available
        break;
    }
  }
  
  private async sendEmailDelivery(result: ExportResult, email: string): Promise<void> {
    // Integration with email service (SendGrid, Postmark, etc.)
    console.log(`Would send email to ${email} with download link: ${result.downloadUrl}`);
  }
  
  private async uploadToS3(
    result: ExportResult,
    config: Record<string, string>
  ): Promise<void> {
    // Integration with AWS S3
    const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
    
    const client = new S3Client({
      region: config.region,
      credentials: {
        accessKeyId: config.accessKeyId,
        secretAccessKey: config.secretAccessKey
      }
    });
    
    // Download file and upload to customer's S3
    const response = await fetch(result.downloadUrl!);
    const buffer = await response.arrayBuffer();
    
    await client.send(new PutObjectCommand({
      Bucket: config.bucket,
      Key: `${config.prefix || ''}${result.fileName}`,
      Body: Buffer.from(buffer)
    }));
  }
  
  private async sendWebhook(result: ExportResult, webhookUrl: string): Promise<void> {
    await fetch(webhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        event: 'export.completed',
        fileName: result.fileName,
        downloadUrl: result.downloadUrl,
        expiresAt: result.expiresAt,
        rowCount: result.rowCount
      })
    });
  }
  
  // ==========================================
  // HELPERS
  // ==========================================
  
  private mapScheduledExport(row: any): ScheduledExport {
    return {
      id: row.id,
      orgId: row.org_id,
      name: row.name,
      schedule: row.schedule,
      exportConfig: row.export_config,
      relativePeriod: row.relative_period,
      deliveryMethod: row.delivery_method,
      deliveryConfig: row.delivery_config,
      enabled: row.enabled,
      lastRunAt: row.last_run_at ? new Date(row.last_run_at) : undefined,
      nextRunAt: row.next_run_at ? new Date(row.next_run_at) : undefined
    };
  }
}

export const usageExportService = new UsageExportService();
```

---
